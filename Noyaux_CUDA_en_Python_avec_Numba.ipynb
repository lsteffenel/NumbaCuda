{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Custom CUDA Kernels in Python with Numba.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/NumbaCuda/blob/main/Noyaux_CUDA_en_Python_avec_Numba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avant de commencer\n",
        "L'exécution de ces notebooks sur Colab nécessite deux choses (au 4/2/2025) :\n",
        "\n",
        "1. des resources GPU\n",
        "  * Menu \"Exécution\" -> \"Modifier le type d'exécution\"\n",
        "2. D'utiliser une version plus ancienne de Colab en raison de certaines incompatibilités du pilote Nvidia\n",
        "  * Menu \"Outils\" -> \"Pallette de commandes\". Cherchez \"version\" dans la barre et sélectionnez l'option \"Utiliser la version d'environnement d'exécution de remplacement\""
      ],
      "metadata": {
        "id": "MBzz9sxb_9B8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PC-hzho2GHi"
      },
      "source": [
        "# Kernels CUDA personnalisés en Python avec Numba\n",
        "\n",
        "Dans cette section, nous approfondirons notre compréhension de la manière dont le modèle de programmation CUDA organise le travail parallèle et nous exploiterons cette compréhension pour écrire des **kernels** CUDA personnalisés, des fonctions qui s'exécutent en parallèle sur les GPU CUDA. Les kernels CUDA personnalisés, en utilisant le modèle de programmation CUDA, nécessitent plus de travail à mettre en œuvre que, par exemple, la simple décoration d'un ufunc avec `@vectorize`. Cependant, ils rendent possible le calcul parallèle dans des endroits où les ufuncs ne le peuvent tout simplement pas, et offrent une flexibilité qui peut conduire au plus haut niveau de performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bUkmWxW2GHk"
      },
      "source": [
        "## Pourquoi créer des Kernels Personnalisés ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEku2lN42GHl"
      },
      "source": [
        "Les fonctions ufuncs sont incroyablement élégantes et, pour toute opération scalaire devant être effectuée élément par élément sur des données, elles constituent probablement l'outil idéal.\n",
        "\n",
        "Comme vous le savez, il existe de nombreuses classes de problèmes, voire plus, qui ne peuvent être résolues en appliquant la même fonction à chaque élément d'un ensemble de données. Considérez, par exemple, tout problème qui nécessite l'accès à plusieurs éléments d'une structure de données afin de calculer sa sortie, comme les algorithmes de pochoir, ou tout problème qui ne peut pas être exprimé par un mappage d'une valeur d'entrée vers une valeur de sortie, comme une réduction. Beaucoup de ces problèmes sont toujours intrinsèquement parallélisables, mais ne peuvent pas être exprimés par une fonction ufunc.\n",
        "\n",
        "L'écriture de kernels CUDA personnalisés, bien que plus difficile que l'écriture d'UFuncs accélérés par GPU, offre aux développeurs une flexibilité considérable pour les types de fonctions qu'ils peuvent envoyer pour s'exécuter en parallèle sur le GPU. De plus, comme vous commencerez à l'apprendre dans cette section et la suivante, il fournit également un contrôle précis sur la manière dont le parallélisme est effectué en exposant explicitement la hiérarchie des threads de CUDA aux développeurs.\n",
        "\n",
        "Bien que nous restions purement en Python, la façon dont nous écrivons les noyaux CUDA à l'aide de Numba rappelle beaucoup la façon dont les développeurs les écrivent dans CUDA C/C++. Pour ceux d'entre vous qui connaissent la programmation en CUDA C/C++, vous apprendrez probablement très rapidement les noyaux personnalisés en Python avec Numba, et pour ceux d'entre vous qui les apprennent pour la première fois, sachez que le travail que vous faites ici vous sera également utile si jamais vous avez besoin ou souhaitez développer CUDA en C/C++, ou même, faire une étude de la richesse des ressources CUDA sur le Web qui représentent le plus souvent le code CUDA C/C++."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiZzw32s2GHm"
      },
      "source": [
        "## Introduction aux kernels CUDA\n",
        "\n",
        "Lors de la programmation en CUDA, les développeurs écrivent des fonctions pour le GPU appelées **kernels** ou **noyaux**, qui sont exécutées, ou dans le jargon CUDA, **lancées**, sur les nombreux cœurs du GPU dans des **threads** parallèles. Lorsque les noyaux sont lancés, les programmeurs utilisent une syntaxe spéciale, appelée **configuration d'exécution** (également appelée configuration de lancement) pour décrire la configuration de l'exécution parallèle.\n",
        "\n",
        "Les diapositives suivantes (qui apparaîtront après l'exécution de la cellule ci-dessous) donnent une introduction de haut niveau sur la façon dont les noyaux CUDA peuvent être créés pour fonctionner sur de grands ensembles de données en parallèle sur le périphérique GPU. Parcourez les diapositives, puis vous commencerez à écrire et à exécuter vos propres noyaux CUDA personnalisés, en utilisant les idées présentées dans les diapositives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "aysOT_cP2GHn"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/AC_CUDA_Python_1.pptx', 640, 390)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiPScyrD2GHp"
      },
      "source": [
        "## Un premier noyau CUDA\n",
        "\n",
        "Commençons par un exemple concret et très simple en réécrivant notre fonction d'addition pour les tableaux NumPy 1D. Les noyaux CUDA sont compilés à l'aide du décorateur `numba.cuda.jit`. `numba.cuda.jit` ne doit pas être confondu avec le décorateur `numba.jit` que vous avez déjà appris et qui optimise les fonctions **pour le CPU**.\n",
        "\n",
        "Nous commencerons par un exemple très simple pour mettre en évidence une partie de la syntaxe essentielle. Il convient de mentionner que cette fonction particulière pourrait en fait être écrite comme un ufunc, mais nous la choisissons ici pour garder l'accent sur l'apprentissage de la syntaxe. Nous passerons ci-dessous à des fonctions plus adaptées à l'écriture en tant que noyau personnalisé. Assurez-vous de lire attentivement les commentaires, car ils fournissent des informations importantes sur le code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "fShxjCwg2GHq"
      },
      "source": [
        "from numba import cuda\n",
        "\n",
        "# Note the use of an `out` array. CUDA kernels written with `@cuda.jit` do not return values,\n",
        "# just like their C counterparts. Also, no explicit type signature is required with @cuda.jit\n",
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "\n",
        "    # The actual values of the following CUDA-provided variables for thread and block indices,\n",
        "    # like function parameters, are not known until the kernel is launched.\n",
        "\n",
        "    # This calculation gives a unique thread index within the entire grid (see the slides above for more)\n",
        "    idx = cuda.grid(1)          # 1 = one dimensional thread grid, returns a single value.\n",
        "                                # This Numba-provided convenience function is equivalent to\n",
        "                                # `cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x`\n",
        "\n",
        "    # This thread will do the work on the data element with the same index as its own\n",
        "    # unique index within the grid.\n",
        "    out[idx] = x[idx] + y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gjd6jzQ22GHr"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "n = 4096\n",
        "x = np.arange(n).astype(np.int32) # [0...4095] on the host\n",
        "y = np.ones_like(x)               # [1...1] on the host\n",
        "\n",
        "d_x = cuda.to_device(x) # Copy of x on the device\n",
        "d_y = cuda.to_device(y) # Copy of y on the device\n",
        "d_out = cuda.device_array_like(d_x) # Like np.array_like, but for device arrays\n",
        "\n",
        "# Because of how we wrote the kernel above, we need to have a 1 thread to one data element mapping,\n",
        "# therefore we define the number of threads in the grid (128*32) to equal n (4096).\n",
        "threads_per_block = 128\n",
        "blocks_per_grid = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "agIOoCa-2GHr"
      },
      "source": [
        "add_kernel[blocks_per_grid, threads_per_block](d_x, d_y, d_out)\n",
        "cuda.synchronize()\n",
        "print(d_out.copy_to_host()) # Should be [1...4096]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3S-exa5e2GHs"
      },
      "source": [
        "### Exercice : modifier le code\n",
        "\n",
        "Apportez les modifications mineures suivantes au code ci-dessus pour voir comment cela affecte son exécution. Faites des suppositions éclairées sur ce qui se passera avant d'exécuter le code :\n",
        "\n",
        "* Diminuez la variable `threads_per_block`\n",
        "* Diminuez la variable `blocks_per_grid`\n",
        "* Augmentez les variables `threads_per_block` et/ou `blocks_per_grid`\n",
        "* Supprimez ou commentez l'appel `cuda.synchronize()`\n",
        "\n",
        "### Résultats\n",
        "\n",
        "Dans l'exemple ci-dessus, étant donné que le noyau est écrit de telle sorte que chaque thread fonctionne sur exactement un élément de données, il est essentiel que le nombre de threads dans la grille soit égal au nombre d'éléments de données.\n",
        "\n",
        "En **réduisant le nombre de threads dans la grille**, soit en réduisant le nombre de blocs, et/ou en réduisant le nombre de threads par bloc, il y a des éléments où le travail n'est pas effectué et nous pouvons donc voir dans la sortie que les éléments vers la fin du tableau `d_out` n'ont pas eu de valeurs ajoutées. Si vous avez modifié la configuration d'exécution en réduisant le nombre de threads par bloc, alors en fait il y a d'autres éléments dans le tableau `d_out` qui n'ont pas été traités.\n",
        "\n",
        "**Augmenter la taille de la grille** crée en fait des problèmes d'accès à la mémoire hors limites. Cette erreur n'apparaîtra pas dans votre code pour le moment, mais plus loin dans cette section, vous apprendrez comment exposer cette erreur à l'aide de `cuda-memcheck` et la déboguer.\n",
        "\n",
        "Vous auriez pu vous attendre à ce que la **suppression du point de synchronisation** ait entraîné une impression indiquant qu'aucun travail ou moins de travail n'avait été effectué. C'est une hypothèse raisonnable puisque sans point de synchronisation, le processeur fonctionnera de manière asynchrone pendant que le GPU est en cours de traitement. Le détail à apprendre ici est que les copies de mémoire portent une synchronisation implicite, rendant l'appel à « cuda.synchronize » ci-dessus inutile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNnFNmic2GHs"
      },
      "source": [
        "### Exercice : accélérer une fonction CPU en tant que noyau CUDA personnalisé\n",
        "\n",
        "Vous trouverez ci-dessous la fonction scalaire CPU `square_device` qui pourrait être utilisée comme ufunc CPU. Votre tâche consiste à la refactoriser pour qu'elle s'exécute en tant que noyau CUDA décoré avec le décorateur `@cuda.jit`.\n",
        "\n",
        "Vous pourriez penser que l'exécution de cette fonction sur l'appareil pourrait être beaucoup plus facile avec `@vectorize`, et vous auriez raison. Mais ce scénario vous donnera l'occasion de travailler avec toute la syntaxe que nous avons introduite avant de passer à des exemples plus complexes et plus réalistes.\n",
        "\n",
        "Dans cet exercice, vous devrez :\n",
        "* Refactoriser la définition `square_device` pour qu'elle soit un noyau CUDA qui effectuera le travail d'un thread sur un seul élément.\n",
        "* Refactoriser les tableaux `d_a` et `d_out` ci-dessous pour qu'ils soient des tableaux de périphériques CUDA.\n",
        "* Modifier les variables `blocks` et `threads` pour qu'elles s'adaptent aux valeurs fournies pour le `n`.\n",
        "* Refactorisez l'appel à `square_device` pour qu'il soit un lancement de noyau incluant une configuration d'exécution.\n",
        "\n",
        "Le test d'assertion ci-dessous échouera jusqu'à ce que vous implémentiez avec succès ce qui précède."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "yESkMIbW2GHt"
      },
      "source": [
        "# Refactor to be a CUDA kernel doing one thread's work.\n",
        "# Don't forget that when using `@cuda.jit`, you must provide an output array as no value will be returned.\n",
        "def square_device(a):\n",
        "    return a**2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Hv-yrX8G2GHt"
      },
      "source": [
        "# Leave the values in this cell fixed for this exercise\n",
        "n = 4096\n",
        "\n",
        "a = np.arange(n)\n",
        "out = a**2 # `out` will only be used for testing below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "0zhOO_-p2GHt"
      },
      "source": [
        "d_a = a                  # TODO make `d_a` a device array\n",
        "d_out = np.zeros_like(a) # TODO: make d_out a device array\n",
        "\n",
        "# TODO: Update the execution configuration for the amount of work needed\n",
        "blocks = 0\n",
        "threads = 0\n",
        "\n",
        "# TODO: Launch as a kernel with an appropriate execution configuration\n",
        "d_out = square_device(d_a)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3Yf4JYfN2GHu"
      },
      "source": [
        "from numpy import testing\n",
        "testing.assert_almost_equal(d_out, out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezzpvmz22GHu"
      },
      "source": [
        "## Une pause pour parler sur les choix de configuration pour la latence et l'exécution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwOsXkln2GHu"
      },
      "source": [
        "Les GPU NVIDIA compatibles CUDA se composent de plusieurs [**Streaming Multiprocessors**](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation), ou **SM** sur une matrice, avec une mémoire DRAM attachée. Les SM contiennent toutes les ressources nécessaires à l'exécution du code du noyau, y compris de nombreux cœurs CUDA. Lorsqu'un noyau est lancé, chaque bloc est attribué à un seul SM, avec potentiellement de nombreux blocs attribués à un seul SM. Les SM partitionnent les blocs en sous-divisions supplémentaires de 32 threads appelées **warps** et ce sont ces warps qui reçoivent des instructions parallèles à exécuter.\n",
        "\n",
        "Lorsqu'une instruction prend plus d'un cycle d'horloge pour se terminer (ou dans le jargon CUDA, pour **expirer**), le SM peut continuer à effectuer un travail significatif *s'il dispose de warps supplémentaires prêts à recevoir de nouvelles instructions.* En raison des fichiers de registre très volumineux sur les SM, il n'y a pas de pénalité de temps pour qu'un SM change de contexte entre l'envoi d'instructions à un warp ou à un autre. En bref, la latence des opérations peut être masquée par les SM avec d'autres tâches significatives tant qu'il y a d'autres tâches à effectuer.\n",
        "\n",
        "**Par conséquent, afin d'exploiter tout le potentiel du GPU et écrire des applications accélérées performantes, il est essentiel de donner aux SM la possibilité de masquer la latence en leur fournissant un nombre suffisant de warps, ce qui peut être accompli le plus simplement possible en exécutant des noyaux avec des dimensions de grille et de bloc suffisamment grandes.**\n",
        "\n",
        "Déterminer la meilleure taille pour la grille de threads CUDA est un problème complexe, et dépend à la fois de l'algorithme et de la [capacité de calcul](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) spécifique du GPU, mais voici quelques heuristiques très approximatives que nous avons tendance à suivre et qui peuvent bien fonctionner pour commencer :\n",
        "\n",
        "* La taille d'un bloc doit être un multiple de 32 threads (la taille d'une chaîne), avec des tailles de bloc typiques comprises entre 128 et 512 threads par bloc.\n",
        "* La taille de la grille doit garantir que le GPU complet est utilisé lorsque cela est possible. Lancer une grille où le nombre de blocs est 2 à 4 fois supérieur au nombre de SM sur le GPU est un bon point de départ. Une plage de 20 à 100 blocs est généralement un bon point de départ.\n",
        "* La surcharge de lancement du noyau CUDA augmente avec le nombre de blocs, donc lorsque la taille d'entrée est très importante, nous trouvons préférable de ne pas lancer une grille où le nombre de threads est égal au nombre d'éléments d'entrée, ce qui entraînerait un nombre énorme de blocs. Au lieu de cela, nous utilisons un modèle sur lequel nous allons maintenant porter notre attention pour gérer les entrées volumineuses."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCkeRH1P2GHv"
      },
      "source": [
        "## Travailler sur les plus grands ensembles de données avec les boucles Grid Stride\n",
        "\n",
        "Les diapositives suivantes donnent un aperçu général d'une technique appelée **boucle Grid Stride** qui permet de créer des noyaux flexibles où chaque thread est capable de travailler sur plusieurs éléments de données, une technique essentielle pour les grands ensembles de données. Exécutez la cellule pour charger les diapositives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Ma4euWJv2GHw"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/AC_CUDA_Python_2.pptx', 640, 390)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuyLUgsr2GHw"
      },
      "source": [
        "## Une première boucle Grid Stride\n",
        "\n",
        "Refactorisons le `add_kernel` ci-dessus pour utiliser une boucle Grid Stride afin de pouvoir le lancer pour travailler sur des ensembles de données plus volumineux de manière flexible tout en bénéficiant des avantages de la **fusion de mémoire** globale, qui permet aux threads parallèles d'accéder à la mémoire par blocs contigus, un scénario que le GPU peut exploiter pour réduire le nombre total d'opérations de mémoire :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "I6fAhYXh2GHw"
      },
      "source": [
        "from numba import cuda\n",
        "\n",
        "@cuda.jit\n",
        "def add_kernel(x, y, out):\n",
        "\n",
        "\n",
        "    start = cuda.grid(1)\n",
        "\n",
        "    # This calculation gives the total number of threads in the entire grid\n",
        "    stride = cuda.gridsize(1)   # 1 = one dimensional thread grid, returns a single value.\n",
        "                                # This Numba-provided convenience function is equivalent to\n",
        "                                # `cuda.blockDim.x * cuda.gridDim.x`\n",
        "\n",
        "    # This thread will start work at the data element index equal to that of its own\n",
        "    # unique index in the grid, and then, will stride the number of threads in the grid each\n",
        "    # iteration so long as it has not stepped out of the data's bounds. In this way, each\n",
        "    # thread may work on more than one data element, and together, all threads will work on\n",
        "    # every data element.\n",
        "    for i in range(start, x.shape[0], stride):\n",
        "        # Assuming x and y inputs are same length\n",
        "        out[i] = x[i] + y[i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GBo7ZpNI2GHx"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "n = 100000 # This is far more elements than threads in our grid\n",
        "x = np.arange(n).astype(np.int32)\n",
        "y = np.ones_like(x)\n",
        "\n",
        "d_x = cuda.to_device(x)\n",
        "d_y = cuda.to_device(y)\n",
        "d_out = cuda.device_array_like(d_x)\n",
        "\n",
        "threads_per_block = 128\n",
        "blocks_per_grid = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "-hIdfs7D2GHx"
      },
      "source": [
        "add_kernel[blocks_per_grid, threads_per_block](d_x, d_y, d_out)\n",
        "print(d_out.copy_to_host()) # Remember, memory copy carries implicit synchronization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fBquTEu2GHx"
      },
      "source": [
        "### Exercice : implémenter une boucle Grid Stride\n",
        "\n",
        "Refactorisez la fonction scalaire CPU `hypot_stride` suivante pour qu'elle s'exécute en tant que noyau CUDA en utilisant une boucle Grid Stride."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "82FU0qJg2GHx"
      },
      "source": [
        "from math import hypot\n",
        "\n",
        "def hypot_stride(a, b, c):\n",
        "    c = hypot(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "H6I8jzCg2GHy"
      },
      "source": [
        "# You do not need to modify the contents in this cell\n",
        "n = 1000000\n",
        "a = np.random.uniform(-12, 12, n).astype(np.float32)\n",
        "b = np.random.uniform(-12, 12, n).astype(np.float32)\n",
        "d_a = cuda.to_device(a)\n",
        "d_b = cuda.to_device(b)\n",
        "d_c = cuda.device_array_like(d_b)\n",
        "\n",
        "blocks = 128\n",
        "threads_per_block = 64\n",
        "\n",
        "hypot_stride[blocks, threads_per_block](d_a, d_b, d_c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "qh4kggYT2GHy"
      },
      "source": [
        "from numpy import testing\n",
        "# This assertion will fail until you successfully implement the hypot_stride kernel above\n",
        "testing.assert_almost_equal(np.hypot(a,b), d_c.copy_to_host(), decimal=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgO2Fk2B2GHy"
      },
      "source": [
        "## Chronométrage du noyau\n",
        "\n",
        "Prenons le temps de faire quelques chronométrages de performances pour le noyau `hypot_stride`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JORQjBVm2GHy"
      },
      "source": [
        "### Base de référence du processeur\n",
        "\n",
        "Commençons par obtenir une base de référence avec `np.hypot` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "JoEUnu8c2GHy"
      },
      "source": [
        "%timeit np.hypot(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU4VsK682GHz"
      },
      "source": [
        "### Numba sur le CPU\n",
        "\n",
        "Voyons maintenant une version optimisée pour le CPU :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bEcUVfbO2GHz"
      },
      "source": [
        "from numba import jit\n",
        "\n",
        "@jit\n",
        "def numba_hypot(a, b):\n",
        "    return np.hypot(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bAiEQVhR2GHz"
      },
      "source": [
        "%timeit numba_hypot(a, b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izAaHjda2GHz"
      },
      "source": [
        "### Thread unique sur l'appareil\n",
        "\n",
        "Juste pour voir, lançons notre noyau dans une grille avec un seul thread. Ici, nous utiliserons `%time`, qui n'exécute l'instruction qu'une seule fois pour garantir que notre mesure n'est pas affectée par la profondeur finie de la file d'attente du noyau CUDA. Nous ajouterons également un `cuda.synchronize` pour être sûrs de ne pas obtenir de temps inexacts en raison du retour du contrôle au processeur, où se trouve le minuteur, avant la fin du noyau :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "JshY1Cn22GH0"
      },
      "source": [
        "%time hypot_stride[1, 1](d_a, d_b, d_c); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5cskxKa2GH0"
      },
      "source": [
        "J'espère que ce n'est pas trop surprenant que cela soit bien plus lent que l'exécution de base du processeur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS0dZpSL2GH0"
      },
      "source": [
        "### Parallelisation dans le GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "jemndrYC2GH0"
      },
      "source": [
        "%time hypot_stride[128, 64](d_a, d_b, d_c); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQJoS6nG2GH0"
      },
      "source": [
        "C'est bien plus rapide !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3x4Wj2R2GH0"
      },
      "source": [
        "## Opérations atomiques et évitement des conditions de concurrence\n",
        "\n",
        "CUDA, comme de nombreux frameworks d'exécution parallèle à usage général, permet d'avoir des conditions de concurrence dans votre code. Une condition de concurrence dans CUDA survient lorsque des threads lisent ou écrivent à partir d'un emplacement mémoire qui peut être modifié par un autre thread indépendant. En règle générale, vous devez vous soucier des :\n",
        "\n",
        "* risques de lecture après écriture : un thread lit un emplacement mémoire en même temps qu'un autre thread peut y écrire.\n",
        "* risques d'écriture après écriture : deux threads écrivent dans le même emplacement mémoire et une seule écriture sera visible lorsque le noyau sera terminé.\n",
        "\n",
        "Une stratégie courante pour éviter ces deux risques consiste à organiser votre algorithme de noyau CUDA de telle sorte que chaque thread ait la responsabilité exclusive de sous-ensembles uniques d'éléments de tableau de sortie et/ou de ne jamais utiliser le même tableau pour l'entrée et la sortie dans un seul appel de noyau. (Les algorithmes itératifs peuvent utiliser une stratégie de double mise en mémoire tampon si nécessaire et changer les tableaux d'entrée et de sortie à chaque itération.)\n",
        "\n",
        "Cependant, il existe de nombreux cas où différents threads doivent combiner les résultats. Considérez quelque chose de très simple, comme : « chaque thread incrémente un compteur global ». L'implémentation de ceci dans votre noyau nécessite que chaque thread :\n",
        "\n",
        "1. Lire la valeur actuelle d'un compteur global.\n",
        "\n",
        "2. Calculer `counter + 1`.\n",
        "\n",
        "3. Réécrire cette valeur dans la mémoire globale.\n",
        "\n",
        "Cependant, il n'y a aucune garantie qu'un autre thread n'ait pas modifié le compteur global entre les étapes 1 et 3. Pour résoudre ce problème, CUDA fournit des **opérations atomiques** qui liront, modifieront et mettront à jour un emplacement mémoire en une seule étape indivisible. Numba prend en charge plusieurs de ces fonctions, [décrites ici](http://numba.pydata.org/numba-doc/dev/cuda/intrinsics.html#supported-atomic-operations).\n",
        "\n",
        "Créons notre noyau de compteur de threads :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3eGkR7Yc2GH1"
      },
      "source": [
        "@cuda.jit\n",
        "def thread_counter_race_condition(global_counter):\n",
        "    global_counter[0] += 1  # This is bad\n",
        "\n",
        "@cuda.jit\n",
        "def thread_counter_safe(global_counter):\n",
        "    cuda.atomic.add(global_counter, 0, 1)  # Safely add 1 to offset 0 in global_counter array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "VmI2eSnV2GH1"
      },
      "source": [
        "# This gets the wrong answer\n",
        "global_counter = cuda.to_device(np.array([0], dtype=np.int32))\n",
        "thread_counter_race_condition[64, 64](global_counter)\n",
        "\n",
        "print('Should be %d:' % (64*64), global_counter.copy_to_host())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WGZcLNVH2GH1"
      },
      "source": [
        "# This works correctly\n",
        "global_counter = cuda.to_device(np.array([0], dtype=np.int32))\n",
        "thread_counter_safe[64, 64](global_counter)\n",
        "\n",
        "print('Should be %d:' % (64*64), global_counter.copy_to_host())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep85kicA2GH1"
      },
      "source": [
        "## Exercice\n",
        "\n",
        "L'exercice suivant vous demandera d'utiliser tout ce que vous avez appris jusqu'à présent. Aucun code de solution ne sera disponible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "A_jgJK252GH2"
      },
      "source": [
        "### Écrire un noyau pour une fonction d'histogramme\n",
        "\n",
        "Pour cette évaluation, vous allez créer un noyau pour un histogramme. Celui-ci prendra un tableau de données d'entrée, une plage et un certain nombre de bacs, et comptera le nombre d'éléments de données d'entrée qui se trouvent dans chaque bac. Vous trouverez ci-dessous une implémentation CPU fonctionnelle de l'histogramme pour servir d'exemple pour votre travail :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cXJM_bNR2GH2"
      },
      "source": [
        "def cpu_histogram(x, xmin, xmax, histogram_out):\n",
        "    '''Increment bin counts in histogram_out, given histogram range [xmin, xmax).'''\n",
        "    # Note that we don't have to pass in nbins explicitly, because the size of histogram_out determines it\n",
        "    nbins = histogram_out.shape[0]\n",
        "    bin_width = (xmax - xmin) / nbins\n",
        "\n",
        "    # This is a very slow way to do this with NumPy, but looks similar to what you will do on the GPU\n",
        "    for element in x:\n",
        "        bin_number = np.int32((element - xmin)/bin_width)\n",
        "        if bin_number >= 0 and bin_number < histogram_out.shape[0]:\n",
        "            # only increment if in range\n",
        "            histogram_out[bin_number] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "5UNzKxjK2GH2"
      },
      "source": [
        "x = np.random.normal(size=10000, loc=0, scale=1).astype(np.float32)\n",
        "xmin = np.float32(-4.0)\n",
        "xmax = np.float32(4.0)\n",
        "histogram_out = np.zeros(shape=10, dtype=np.int32)\n",
        "\n",
        "cpu_histogram(x, xmin, xmax, histogram_out)\n",
        "\n",
        "histogram_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYDZ3kr52GH2"
      },
      "source": [
        "À l’aide d’une boucle Grid Stride et d’opérations atomiques, implémentez votre solution dans la cellule ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "y2roDrKn2GH3"
      },
      "source": [
        "@cuda.jit\n",
        "def cuda_histogram(x, xmin, xmax, histogram_out):\n",
        "    '''Increment bin counts in histogram_out, given histogram range [xmin, xmax).'''\n",
        "\n",
        "    pass  # Replace this with your implementation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "T5lS1U7Q2GH3"
      },
      "source": [
        "d_x = cuda.to_device(x)\n",
        "d_histogram_out = cuda.to_device(np.zeros(shape=10, dtype=np.int32))\n",
        "\n",
        "blocks = 128\n",
        "threads_per_block = 64\n",
        "\n",
        "cuda_histogram[blocks, threads_per_block](d_x, xmin, xmax, d_histogram_out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Db22nWsQ2GH3"
      },
      "source": [
        "# This assertion will fail until you correctly implement `cuda_histogram`\n",
        "np.testing.assert_array_almost_equal(d_histogram_out.copy_to_host(), histogram_out, decimal=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gardez votre réponse, elle pourra être soumise pour la certification NVIDIA."
      ],
      "metadata": {
        "id": "nQ5fd_wvD7ky"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zLzqHCHEDBS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}