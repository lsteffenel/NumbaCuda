{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Effective+Memory+Use.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/NumbaCuda/blob/main/Effective_Memory_Use.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avant de commencer\n",
        "L'ex√©cution de ces notebooks sur Colab n√©cessite deux choses (au 4/2/2025) :\n",
        "\n",
        "1. des resources GPU\n",
        "  * Menu \"Ex√©cution\" -> \"Modifier le type d'ex√©cution\"\n",
        "2. D'utiliser une version plus ancienne de Colab en raison de certaines incompatibilit√©s du pilote Nvidia\n",
        "  * Connecter l'environnement d'ex√©cution\n",
        "  * Menu \"Outils\" -> \"Pallette de commandes\". Cherchez \"version\" dans la barre et s√©lectionnez l'option \"Utiliser la version d'environnement d'ex√©cution de remplacement\"\n"
      ],
      "metadata": {
        "id": "qcfzgaA2mW7U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVtAcH6Q0Mna"
      },
      "source": [
        "# Utilisation efficace du sous-syst√®me de m√©moire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpyt3gzT0Mnb"
      },
      "source": [
        "Maintenant que vous savez √©crire des noyaux CUDA  et que vous comprenez l'importance de lancer des grilles pour donner suffisamment de travail au GPU afin de masquer la latence, vous allez apprendre des techniques pour utiliser efficacement la m√©moire du GPU. Ces techniques sont largement applicables √† une vari√©t√© d'applications CUDA et sont parmi les plus importantes lorsqu'il s'agit d'acc√©l√©rer votre code CUDA.\n",
        "\n",
        "Vous allez commencer par en apprendre davantage sur la coalescence de m√©moire (regroupement/organisation de blocs m√©moire). Pour tester votre capacit√© √† raisonner sur la coalescence, vous d√©couvrirez ensuite les grilles bidimensionnelles et les blocs de threads. Ensuite, vous d√©couvrirez comment utiliser la m√©moire partag√©e, qui sera utilis√©e pour faciliter la coalescence l√† o√π cela n'aurait pas √©t√© possible autrement. Enfin, vous d√©couvrirez les conflits qui peuvent arriver avec la m√©moire partag√©e et une technique pour les r√©soudre.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3GIL0le0Mnd"
      },
      "source": [
        "## Le probl√®me : l'acc√®s \"√©parpill√©\" √† la m√©moire nuit les performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loksj-jY0Mnd"
      },
      "source": [
        "Avant d‚Äôapprendre les d√©tails sur la coalescence, ex√©cutez les cellules suivantes pour observer les implications en termes de performances d‚Äôun changement apparemment trivial du mode d‚Äôacc√®s aux donn√©es."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--TayTTy0Mnd"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rMOxcJrS0Mne"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DchcEDY10Mng"
      },
      "source": [
        "### Data Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbUfmnzj0Mng"
      },
      "source": [
        "Dans cette cellule, nous d√©finissons `n` et cr√©ons une grille avec `n` threads. Nous cr√©ons √©galement un vecteur de sortie de longueur `n`. Pour les entr√©es, nous cr√©ons des vecteurs de taille `stride * n` pour des raisons qui seront expliqu√©es ci-dessous :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "sSIYp-Rl0Mnh"
      },
      "source": [
        "n = 1024*1024 # 1M\n",
        "\n",
        "threads_per_block = 1024\n",
        "blocks = int(n / threads_per_block)\n",
        "\n",
        "stride = 16\n",
        "\n",
        "# Input Vectors of length stride * n\n",
        "a = np.ones(stride * n).astype(np.float32)\n",
        "b = a.copy().astype(np.float32)\n",
        "\n",
        "# Output Vector\n",
        "out = np.zeros(n).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_b = cuda.to_device(b)\n",
        "d_out = cuda.to_device(out)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuktrimW0Mnh"
      },
      "source": [
        "### Kernel Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0JmJ9l60Mnh"
      },
      "source": [
        "Dans `add_experiment`, chaque thread de la grille ajoutera un √©l√©ment dans `a` et un √©l√©ment dans `b` puis √©crira le r√©sultat dans `out`. Le noyau a √©t√© √©crit de telle sorte que nous puissions passer une valeur `coalesced` de `True` ou `False` pour affecter la fa√ßon dont il indexe dans les vecteurs `a` et `b`. Vous verrez la comparaison des performances des deux modes ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "J1pNj0RP0Mni"
      },
      "source": [
        "@cuda.jit\n",
        "def add_experiment(a, b, out, stride, coalesced):\n",
        "    i = cuda.grid(1)\n",
        "    # The above line is equivalent to\n",
        "    # i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    if coalesced == True:\n",
        "        out[i] = a[i] + b[i]\n",
        "    else:\n",
        "        out[i] = a[stride*i] + b[stride*i]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tov38eY0Mni"
      },
      "source": [
        "### Lancement d'un kernet avec un acc√®s \"coalesced\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EzN1lvF0Mnj"
      },
      "source": [
        "Ici, nous passons ¬´ True ¬ª comme valeur ¬´ coalesced ¬ª et observons les performances du noyau sur plusieurs ex√©cutions¬†:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "qP_h6zzm0Mnj",
        "outputId": "b4fb63cb-58d4-4707-d10c-688a2cfcf94f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, stride, True); cuda.synchronize"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91.5 ¬µs ¬± 44.8 ¬µs per loop (mean ¬± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94KnxXDH0Mnj"
      },
      "source": [
        "V√©rifions si le noyau s'ex√©cute comme attendu :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "E7ilWl6D0Mnk"
      },
      "source": [
        "result = d_out.copy_to_host()\n",
        "truth = a[:n] + b[:n]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "9U02BdKP0Mnk",
        "outputId": "25d0679a-ec7e-43ca-c1b9-3c669f314d9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array_equal(result, truth)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkMjgEWq0Mnk"
      },
      "source": [
        "### Lancement d'un noyau sans acc√®s coalescent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYiuF8ZR0Mnk"
      },
      "source": [
        "Dans cette cellule, nous passons \"¬†False¬†\" pour observer les performances du mod√®le d'acc√®s aux donn√©es non coalescents¬†:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HY9oGSDt0Mnl",
        "outputId": "0d546575-2b2c-46a9-f5e8-053d34026208",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, stride, False); cuda.synchronize"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "536 ¬µs ¬± 136 ns per loop (mean ¬± std. dev. of 7 runs, 10000 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VodDON60Mnl"
      },
      "source": [
        "V√©rifions si le noyau s'ex√©cute comme attendu :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "e4yAsZOD0Mnl"
      },
      "source": [
        "result = d_out.copy_to_host()\n",
        "truth = a[::stride] + b[::stride]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UgxF3vkt0Mnl",
        "outputId": "c519dc39-0981-4a80-ef39-5cea93f4c1db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array_equal(result, truth)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6K4dg590Mnl"
      },
      "source": [
        "### R√©sultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVKtxTET0Mnm"
      },
      "source": [
        "Les performances du mode d'acc√®s \"non coalescent\" sont bien pires. Vous allez maintenant d√©couvrir pourquoi et comment r√©fl√©chir aux modes d'acc√®s aux donn√©es pour obtenir des noyaux tr√®s performants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y7yB1vg0Mnm"
      },
      "source": [
        "## Pr√©sentation : Global Memory Coalescing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV0LZIyR0Mnm"
      },
      "source": [
        "Regardez la pr√©sentation ci-dessous :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7PYWV1wY0Mnm",
        "outputId": "39956885-7f3c-4153-968d-a3cb2623dc23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        }
      },
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/coalescing-v3.pptx', 800, 450)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7aacbc27a5f0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"800\"\n",
              "            height=\"450\"\n",
              "            src=\"https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/coalescing-v3.pptx\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEVtOBbR0Mnm"
      },
      "source": [
        "## Exercice : Somme des Colonnes et Lignes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBhLKf6u0Mnn"
      },
      "source": [
        "Pour cet exercice, il vous sera demand√© d'√©crire un noyau pour faire la somme des colonnes, utilisant le mode d'acc√®s m√©moire coalesc√©s. Pour commencer, vous observerez les performances sans ce mode d' acc√®s m√©moire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa9jRAJI0Mnn"
      },
      "source": [
        "### Somme des lignes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-bKCcyD0Mnn"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "yIVrXcGn0Mnn"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDlkLI2p0Mnn"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQL4dpWq0Mnn"
      },
      "source": [
        "Dans ce paragraphe nous cr√©ons une matrice pour l'entr√©e ainsi qu'un vecteur pour stocker la solution, et nous transf√©rons chacun d'eux vers le p√©riph√©rique. Nous d√©finissons √©galement les dimensions de la grille et du bloc √† utiliser lorsque nous lan√ßons le noyau.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Sou7fNbf0Mno"
      },
      "source": [
        "n = 16384 # matrix side size\n",
        "threads_per_block = 256\n",
        "blocks = int(n / threads_per_block)\n",
        "\n",
        "# Input Matrix\n",
        "a = np.ones(n*n).reshape(n, n).astype(np.float32)\n",
        "# Here we set an arbitrary row to an arbitrary value to facilitate a check for correctness below.\n",
        "a[3] = 9\n",
        "\n",
        "# Output vector\n",
        "sums = np.zeros(n).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_sums = cuda.to_device(sums)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQYPTiQT0Mno"
      },
      "source": [
        "**Le noyau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xibILyND0Mno"
      },
      "source": [
        "`row_sums` utilisera chaque thread pour parcourir une ligne de donn√©es, effectuer la somme, puis stockera la somme des lignes dans `sums`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "67woR5pO0Mno"
      },
      "source": [
        "@cuda.jit\n",
        "def row_sums(a, sums, n):\n",
        "    idx = cuda.grid(1)\n",
        "    sum = 0.0\n",
        "\n",
        "    for i in range(n):\n",
        "        # Each thread will sum a row of `a`\n",
        "        sum += a[idx][i]\n",
        "\n",
        "    sums[idx] = sum"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee57AO4_0Mno"
      },
      "source": [
        "**Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "TZS_9bfZ0Mno",
        "outputId": "1fde499d-48b1-476d-bd6d-60420ff2d27a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%timeit row_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 64 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.6 ms ¬± 206 ¬µs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLicTRw0Mnp"
      },
      "source": [
        "**V√©rification du r√©sultat**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rUsKLQMi0Mnp"
      },
      "source": [
        "result = d_sums.copy_to_host()\n",
        "truth = a.sum(axis=1)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cajPu6xc0Mnp",
        "outputId": "049dea39-05fe-4874-adde-c67a2e804601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.array_equal(truth, result)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ofbySFG0Mnp"
      },
      "source": [
        "### Somme des colonnes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OckAKRIG0Mnp"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HohrB1RC0Mnp"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJpUHYgy0Mnp"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eNsBt1w0Mnq"
      },
      "source": [
        "On reprend le m√™me format pr√©c√©dent, mais avec des valeurs sur les colonnes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cemu8-A-0Mnq"
      },
      "source": [
        "n = 16384 # matrix side size\n",
        "threads_per_block = 256\n",
        "blocks = int(n / threads_per_block)\n",
        "\n",
        "a = np.ones(n*n).reshape(n, n).astype(np.float32)\n",
        "# Here we set an arbitrary column to an arbitrary value to facilitate a check for correctness below.\n",
        "a[:, 3] = 9\n",
        "sums = np.zeros(n).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_sums = cuda.to_device(sums)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vQMYSju0Mnq"
      },
      "source": [
        "**D√©finition du noyau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXuf1JVr0Mnq"
      },
      "source": [
        "`col_sums` utilisera chaque thread pour parcourir une colonne de donn√©es, en la sommant, puis stockera la somme de sa colonne dans `sums`. Compl√©tez la d√©finition du noyau pour y parvenir (c'est √† vous de le faire üòÄ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vSBaE_qy0Mnq"
      },
      "source": [
        "@cuda.jit\n",
        "def col_sums(a, sums, ds):\n",
        "    # TODO: Write this kernel to store the sum of each column in matrix `a` to the `sums` vector.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i15Fke5q0Mnq"
      },
      "source": [
        "**V√©rification de la Performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQgYMZx30Mnr"
      },
      "source": [
        "En supposant que vous ayez √©crit `col_sums` pour utiliser l'acc√®s coalescent, vous devriez voir une acc√©l√©ration significative (presque 2x) par rapport aux `row_sums` \"non coalescent\" que vous avez ex√©cut√©s ci-dessus¬†:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "f35hHNfm0Mnr",
        "outputId": "6919bd0f-bf2e-46f5-e779-5c9346d1d1f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "%timeit col_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'col_sums' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-6ddf460b6976>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'col_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1178\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'col_sums' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfgNd_IL0Mnr"
      },
      "source": [
        "**V√©rification des r√©sultats**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIYJS3FY0Mnr"
      },
      "source": [
        "Confirm your kernel is working as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "y9qXj5Kl0Mnr"
      },
      "source": [
        "result = d_sums.copy_to_host()\n",
        "truth = a.sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "7ytHoxap0Mnr"
      },
      "source": [
        "np.array_equal(truth, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY-RQRW10Mnr"
      },
      "source": [
        "## Des blocs et grilles √† 2 et 3 dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9bCiskf0Mns"
      },
      "source": [
        "Les grilles et les blocs peuvent √™tre configur√©s pour contenir respectivement une collection bidimensionnelle ou tridimensionnelle de blocs ou de threads. Cela est fait principalement pour des raisons de commodit√© pour les programmeurs qui travaillent avec des donn√©es bidimensionnels ou tridimensionnels. Voici un exemple tr√®s simple pour mettre en √©vidence la syntaxe. Il faudra comprendre la d√©finition du noyau et comme il est lanc√© pour que le concept n'ait un sens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MoYug8R90Mns"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rH9lgSHA0Mns"
      },
      "source": [
        "A = np.zeros((4,4)) # A 4x4 Matrix of 0's\n",
        "d_A = cuda.to_device(A)\n",
        "\n",
        "# Here we create a 2D grid with 4 blocks in a 2x2 structure, each with 4 threads in a 2x2 structure\n",
        "# by using a Python tuple to signify grid and block dimensions.\n",
        "blocks = (2, 2)\n",
        "threads_per_block = (2, 2)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4kDlvZo0Mns"
      },
      "source": [
        "Ce noyau prendra une matrice d'entr√©e de 0 et √©crira chacun de ses √©l√©ments directement dans la grille au format `X.Y`` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IxghwLch0Mns"
      },
      "source": [
        "@cuda.jit\n",
        "def get_2D_indices(A):\n",
        "    # By passing `2`, we get the thread's unique x and y coordinates in the 2D grid\n",
        "    x, y = cuda.grid(2)\n",
        "    # The above is equivalent to the following 2 lines of code:\n",
        "    # x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    # y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
        "\n",
        "    # Write the x index followed by a decimal and the y index.\n",
        "    A[x][y] = x + y / 10"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RZtWAc4c0Mns",
        "outputId": "5b700f90-c4b2-4d98-8fa6-4eea420f1c35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "get_2D_indices[blocks, threads_per_block](d_A)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numba/cuda/dispatcher.py:536: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3Dytd6qd0Mnt",
        "outputId": "97b85c98-9782-4725-f39a-c46e3871537b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = d_A.copy_to_host()\n",
        "result"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0. , 0.1, 0.2, 0.3],\n",
              "       [1. , 1.1, 1.2, 1.3],\n",
              "       [2. , 2.1, 2.2, 2.3],\n",
              "       [3. , 3.1, 3.2, 3.3]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNPnk2YL0Mnt"
      },
      "source": [
        "## Exercice : Somme de matrices 2D en mode coalescent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UytTSf790Mnt"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "SatpVW8U0Mnt"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CcT0xG90Mnt"
      },
      "source": [
        "### Data Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVciXU660Mnt"
      },
      "source": [
        "Dans cette cellule, nous d√©finissons des matrices d'entr√©e d'√©l√©ments 2048x2048 `a` et `b`, ainsi qu'une matrice de sortie initialis√©e de 2048x2048. Nous copions ces matrices sur le GPU.\n",
        "\n",
        "Nous d√©finissons √©galement les dimensions de bloc et de grille √† 2 dimensions. Notez que nous cr√©ons une grille avec le m√™me nombre total de threads que d'√©l√©ments d'entr√©e et de sortie, de sorte que chaque thread de la grille calculera la somme pour un seul √©l√©ment de la matrice de sortie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "q_TLDr1Z0Mnu"
      },
      "source": [
        "n = 2048*2048 # 4M\n",
        "\n",
        "# 2D blocks\n",
        "threads_per_block = (32, 32)\n",
        "# 2D grid\n",
        "blocks = (64, 64)\n",
        "\n",
        "# 2048x2048 input matrices\n",
        "a = np.arange(n).reshape(2048,2048).astype(np.float32)\n",
        "b = a.copy().astype(np.float32)\n",
        "\n",
        "# 2048x2048 0-initialized output matrix\n",
        "out = np.zeros_like(a).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_b = cuda.to_device(b)\n",
        "d_out = cuda.to_device(out)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eFw5ZJu0Mnu"
      },
      "source": [
        "### Somme pour une matrice 2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l05Yos500Mnu"
      },
      "source": [
        "Votre t√¢che consiste √† compl√©ter les t√¢ches √† effectuer dans `matrix_add` pour additionner correctement `a` et `b` dans `out`. Pour vous aider √† comprendre les modes d'acc√®s, `matrix_add` acceptera un bool√©en `coalesced` indiquant si les mod√®les d'acc√®s doivent √™tre coalescents ou non. Les deux modes (coalesced et uncoalesced) devraient produire des r√©sultats corrects, cependant, vous devriez observer des acc√©l√©rations significatives ci-dessous lors de l'ex√©cution avec `coalesced` d√©fini sur `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "V4uR7ezk0Mnu"
      },
      "source": [
        "@cuda.jit\n",
        "def matrix_add(a, b, out, coalesced):\n",
        "    # TODO: set x and y to index correctly such that each thread\n",
        "    # accesses one element in the data.\n",
        "    x, y = pass\n",
        "\n",
        "    if coalesced == True:\n",
        "        # TODO: write the sum of one element in `a` and `b` to `out`\n",
        "        # using a coalesced memory access pattern.\n",
        "    else:\n",
        "        # TODO: write the sum of one element in `a` and `b` to `out`\n",
        "        # using an uncoalesced memory access pattern."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMzVBdpm0Mnv"
      },
      "source": [
        "### V√©rification de la performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wEdq7G50Mnv"
      },
      "source": [
        "Ex√©cutez les deux cellules ci-dessous pour lancer `matrix_add` avec les mod√®les d'acc√®s que vous avez √©crits, et observez la diff√©rence de performances. Des cellules suppl√©mentaires ont √©t√© fournies pour confirmer l'exactitude de votre noyau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMLqgTX30Mnv"
      },
      "source": [
        "**Coalesced**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "OpvsdKTy0Mnv"
      },
      "source": [
        "%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, True); cuda.synchronize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Z0WO3j2F0Mnw"
      },
      "source": [
        "result = d_out.copy_to_host()\n",
        "truth = a+b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "smvzVIio0Mnw"
      },
      "source": [
        "np.array_equal(result, truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPRV7EZR0Mnw"
      },
      "source": [
        "**Uncoalesced**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tsCvAmBI0Mnw"
      },
      "source": [
        "%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, False); cuda.synchronize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GaEAgTi_0Mnx"
      },
      "source": [
        "result = d_out.copy_to_host()\n",
        "truth = a+b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "J9SAA83P0Mnx"
      },
      "source": [
        "np.array_equal(result, truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwXlyrZT0Mnx"
      },
      "source": [
        "## M√©moire Partag√©e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dse3CL4a0Mnx"
      },
      "source": [
        "So far we have been differentiating between host and device memory, as if device memory were a single kind of memory. But in fact, CUDA has an even more fine-grained [memory hierarchy](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy). The device memory we have been utilizing thus far is called **global memory** which is available to any thread or block on the device, can persist for the lifetime of the application, and is a relatively large memory space.\n",
        "\n",
        "We will now discuss how to utilize a region of on-chip device memory called **shared memory**. Shared memory is a programmer defined cache of limited size that [depends on the GPU](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities) being used and is **shared** between all threads in a block. It is a scarce resource, cannot be accessed by threads outside of the block where it was allocated, and does not persist after a kernel finishes executing. Shared memory however has a much higher bandwidth than global memory and can be used to great effect in many kernels, especially to optimize performance.\n",
        "\n",
        "Here are a few common use cases for shared memory:\n",
        "\n",
        " * Caching memory read from global memory that will need to be read multiple times within a block.\n",
        " * Buffering output from threads so it can be coalesced before writing it back to global memory.\n",
        " * Staging data for scatter/gather operations within a block."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44wmMLl40Mnx"
      },
      "source": [
        "### Shared Memory Syntax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rvCoReK0Mnx"
      },
      "source": [
        "Numba provides [functions](https://numba.pydata.org/numba-doc/dev/cuda/memory.html#shared-memory-and-thread-synchronization) for allocating shared memory as well as for synchronizing between threads in a block, which is often necessary after parallel threads read from or write to shared memory.\n",
        "\n",
        "When declaring shared memory, you provide the shape of the shared array, as well as its type, using a [Numba type](https://numba.pydata.org/numba-doc/dev/reference/types.html#numba-types). **The shape of the array must be a constant value**, and therefore, you cannot use arguments passed into the function, or, provided variables like `numba.cuda.blockDim.x`, or the calculated values of `cuda.griddim`. Here is a convoluted example to demonstrate the syntax with comments pointing out the movement from host memory to global device memory, to shared memory, back to global device memory, and finally back to host memory:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhH49D360Mny"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFzMbDi0Mny"
      },
      "source": [
        "We will use `numba.types` to define the types of values in shared memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8CpCtKmq0Mny"
      },
      "source": [
        "import numpy as np\n",
        "from numba import types, cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjLTtBxd0Mny"
      },
      "source": [
        "**Swap Elements Using Shared Memory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VENFAVZZ0Mny"
      },
      "source": [
        "The following kernel takes an input vector, where each thread will first write one element of the vector to shared memory, and then, after syncing such that all elements have been written to shared memory, will write one element out of shared memory into the swapped output vector.\n",
        "\n",
        "Worth noting is that each thread will be writing a swapped value from shared memory that was written into shared memory by another thread."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xko3UNwG0Mny"
      },
      "source": [
        "@cuda.jit\n",
        "def swap_with_shared(vector, swapped):\n",
        "    # Allocate a 4 element vector containing int32 values in shared memory.\n",
        "    temp = cuda.shared.array(4, dtype=types.int32)\n",
        "\n",
        "    idx = cuda.grid(1)\n",
        "\n",
        "    # Move an element from global memory into shared memory\n",
        "    temp[idx] = vector[idx]\n",
        "\n",
        "    # cuda.syncthreads will force all threads in the block to synchronize here, which is necessary because...\n",
        "    cuda.syncthreads()\n",
        "    #...the following operation is reading an element written to shared memory by another thread.\n",
        "\n",
        "    # Move an element from shared memory back into global memory\n",
        "    swapped[idx] = temp[3 - cuda.threadIdx.x] # swap elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ-9_3ow0Mny"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7Mr2lOMY0Mnz"
      },
      "source": [
        "vector = np.arange(4).astype(np.int32)\n",
        "swapped = np.zeros_like(vector)\n",
        "\n",
        "# Move host memory to device (global) memory\n",
        "d_vector = cuda.to_device(vector)\n",
        "d_swapped = cuda.to_device(swapped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "6aLXu-JS0Mnz"
      },
      "source": [
        "vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DPD7p3S0Mnz"
      },
      "source": [
        "** Run Kernel**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FxwDr5M70Mnz"
      },
      "source": [
        "swap_with_shared[1, 4](d_vector, d_swapped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St56M2zB0Mnz"
      },
      "source": [
        "**Check Results**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dpICY2U80Mnz"
      },
      "source": [
        "# Move device (global) memory back to the host\n",
        "result = d_swapped.copy_to_host()\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S8GEvKz0Mnz"
      },
      "source": [
        "## Presentation: Shared Memory for Memory Coalescing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKdZZRjl0Mn0"
      },
      "source": [
        "Execute the following cell to load the slides, then click on \"Start Slide Show\" to make them full screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "JNpWnV4L0Mn0"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/shared_coalescing.pptx', 800, 450)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gT6yvrB0Mn0"
      },
      "source": [
        "## Exercise: Used Shared Memory for Coalesced Reads and Writes With Matrix Transpose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_uoQDSk0Mn0"
      },
      "source": [
        "In this exercise you will implement what was just demonstrated in the presentation by writing a matrix transpose kernel which, using shared memory, makes coalesced reads and writes to the output matrix in global memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5Nar1Os0Mn0"
      },
      "source": [
        "### Coalesced Reads, Uncoalesced Writes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Zzf6sE0Mn0"
      },
      "source": [
        "As reference, and for performance comparison, here is a naive matrix transpose kernel that makes coalesced reads from input, but uncoalesced writes to output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOZH4ME10Mn0"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GxUeJafe0Mn1"
      },
      "source": [
        "from numba import cuda\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2qqSTzg0Mn1"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6I8hVQ-0Mn1"
      },
      "source": [
        "Here we create a 4096x4096 input matrix `a` as well as a 4096x4096 output matrix `transposed`, and copy them to the device.\n",
        "\n",
        "We also define a 2-dimensional grid with 2-dimensional blocks to be used below. Note that we have created a grid with a total number of threads equal to the number of elments in the input matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "A4dvUTyq0Mn1"
      },
      "source": [
        "n = 4096*4096 # 16M\n",
        "\n",
        "# 2D blocks\n",
        "threads_per_block = (32, 32)\n",
        "#2D grid\n",
        "blocks = (128, 128)\n",
        "\n",
        "# 4096x4096 input and output matrices\n",
        "a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n",
        "transposed = np.zeros_like(a).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_transposed = cuda.to_device(transposed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfcahheb0Mn1"
      },
      "source": [
        "**Naive Matrix Transpose Kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imgIJyzh0Mn1"
      },
      "source": [
        "This kernel correctly transposes `a`, writing the transposition to `transposed`. It makes reads from `a` in a coalesced fashion, however, its writes to `transposed` are uncoalesced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "T7USDX080Mn1"
      },
      "source": [
        "@cuda.jit\n",
        "def transpose(a, transposed):\n",
        "    x, y = cuda.grid(2)\n",
        "\n",
        "    transposed[x][y] = a[y][x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE6C-mU50Mn2"
      },
      "source": [
        "**Check Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bN28INj20Mn2"
      },
      "source": [
        "%timeit transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyye0Iye0Mn2"
      },
      "source": [
        "**Check Correctness**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gMhV_Yar0Mn2"
      },
      "source": [
        "result = d_transposed.copy_to_host()\n",
        "expected = a.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "pw_cFWke0Mn2"
      },
      "source": [
        "np.array_equal(result, expected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CxXi3jr0Mn2"
      },
      "source": [
        "### Refactor for Coalesced Reads and Writes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVQtAOPX0Mn2"
      },
      "source": [
        "Your job will be to refactor the `transpose` kernel to use shared memory and make both reads to and writes from global memory in a coalesced fashion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTiJpAky0Mn2"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "w0SlRnJA0Mn3"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda, types as numba_types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovwo4JMi0Mn3"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WkHRO5Ko0Mn3"
      },
      "source": [
        "n = 4096*4096 # 16M\n",
        "\n",
        "# 2D blocks\n",
        "threads_per_block = (32, 32)\n",
        "#2D grid\n",
        "blocks = (128, 128)\n",
        "\n",
        "# 4096x4096 input and output matrices\n",
        "a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n",
        "transposed = np.zeros_like(a).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_transposed = cuda.to_device(transposed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxjIWxd70Mn3"
      },
      "source": [
        "**Write a Transpose Kernel that Uses Shared Memory**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSvTbmTZ0Mn3"
      },
      "source": [
        "Complete the TODOs inside the `tile_transpose` kernel definition.\n",
        "\n",
        "If you get stuck, feel free to check out [the solution](section3/solutions/tile_transpose_solution.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ts-KOdVT0Mn3"
      },
      "source": [
        "@cuda.jit\n",
        "def tile_transpose(a, transposed):\n",
        "    # `tile_transpose` assumes it is launched with a 32x32 block dimension,\n",
        "    # and that `a` is a multiple of these dimensions.\n",
        "\n",
        "    # 1) Create 32x32 shared memory array.\n",
        "\n",
        "    # TODO: Your code here.\n",
        "\n",
        "    # Compute offsets into global input array. Recall for coalesced access we want to map threadIdx.x increments to\n",
        "    # the fastest changing index in the data, i.e. the column in our array.\n",
        "    # Note: `a_col` and `a_row` are already correct.\n",
        "    a_col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    a_row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
        "\n",
        "    # 2) Make coalesced read from global memory (using grid indices)\n",
        "    # into shared memory array (using thread indices).\n",
        "\n",
        "    # TODO: Your code here.\n",
        "\n",
        "    # 3) Wait for all threads in the block to finish updating shared memory.\n",
        "\n",
        "    # TODO: Your code here.\n",
        "\n",
        "    # 4) Calculate transposed location for the shared memory array tile\n",
        "    # to be written back to global memory. Note that blockIdx.y*blockDim.y\n",
        "    # and blockIdx.x* blockDim.x are swapped (because we want to write to the\n",
        "    # transpose locations), but we want to keep access coalesced, so match up the\n",
        "    # threadIdx.x to the fastest changing index, i.e. the column./\n",
        "    # Note: `t_col` and `t_row` are already correct.\n",
        "    t_col = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n",
        "    t_row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n",
        "\n",
        "    # 5) Write from shared memory (using thread indices)\n",
        "    # back to global memory (using grid indices)\n",
        "    # transposing each element within the shared memory array.\n",
        "\n",
        "    # TODO: Your code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlIfrROr0Mn4"
      },
      "source": [
        "**Check Performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8CNWMlg0Mn4"
      },
      "source": [
        "Check the performance of your refactored transpose kernel. You should see a speedup compared to the baseline transpose performance above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "I-dA5i0X0Mn4"
      },
      "source": [
        "%timeit tile_transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqv47IKI0Mn4"
      },
      "source": [
        "**Check Correctness**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1GsR6Y_u0Mn4"
      },
      "source": [
        "result = d_transposed.copy_to_host()\n",
        "expected = a.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "VnbEeyFH0Mn4"
      },
      "source": [
        "np.array_equal(result, expected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InCDnwgs0Mn4"
      },
      "source": [
        "### Why Such a Small Improvement?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlfwNhul0Mn4"
      },
      "source": [
        "While this is a significant speedup for only a few lines of code, but you might think that the performance improvement is not as stark as you expected based on earlier performance improvements to use coalesced access patterns. There are 2 main reasons for this:\n",
        "\n",
        "1. The naive transpose kernel was making coalesced reads, so, your refactored version only optimized half of the global memory access throughout the execution of the kernel.\n",
        "2. Your code as written suffers from something called shared memory bank conflicts, a topic to which we will now turn our attention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42jEkFzX0Mn5"
      },
      "source": [
        "## Presentation: Memory Bank Conflicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3bV4pX90Mn5"
      },
      "source": [
        "Execute the following cell to load the slides, then click on \"Start Slide Show\" to make them full screen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rA-Fd-8Z0Mn5"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/bank_conflicts.pptx', 800, 450)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJL6U1Mf0Mn5"
      },
      "source": [
        "## Exercice: Resolve Memory Bank Conflicts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be8-quUE0Mn5"
      },
      "source": [
        "As a final exercise, you will refactor the transpose kernel utilizing shared memory to be shared memory bank conflict free."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH45i_aH0Mn6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vFnPfMgt0Mn6"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda, types as numba_types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1x8xjYU0Mn6"
      },
      "source": [
        "### Data Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "j2HJNvJt0Mn6"
      },
      "source": [
        "n = 4096*4096 # 16M\n",
        "threads_per_block = (32, 32)\n",
        "blocks = (128, 128)\n",
        "\n",
        "a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n",
        "transposed = np.zeros_like(a).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_transposed = cuda.to_device(transposed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAz79gP_0Mn6"
      },
      "source": [
        "### Make the Kernel Bank Conflict Free"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB2hn9kY0Mn6"
      },
      "source": [
        "The `tile_transpose_conflict_free` kernel is a working matrix transpose kernel which utilizes shared memory so that both reads from and writes to global memory are coalesced. Your job is to refactor the kernel so that it does not suffer from memory bank conflicts.\n",
        "\n",
        "**Note:**  a solution will not be provided."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cz-EYhxZ0Mn6"
      },
      "source": [
        "@cuda.jit\n",
        "def tile_transpose_conflict_free(a, transposed):\n",
        "    # `tile_transpose` assumes it is launched with a 32x32 block dimension,\n",
        "    # and that `a` is a multiple of these dimensions.\n",
        "\n",
        "    # 1) Create 32x32 shared memory array.\n",
        "    tile = cuda.shared.array((32, 32), numba_types.int32)\n",
        "\n",
        "    # Compute offsets into global input array.\n",
        "    x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
        "\n",
        "    # 2) Make coalesced read from global memory into shared memory array.\n",
        "    # Note the use of local thread indices for the shared memory write,\n",
        "    # and global offsets for global memory read.\n",
        "    tile[cuda.threadIdx.y, cuda.threadIdx.x] = a[y, x]\n",
        "\n",
        "    # 3) Wait for all threads in the block to finish updating shared memory.\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # 4) Calculate transposed location for the shared memory array tile\n",
        "    # to be written back to global memory.\n",
        "    t_x = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n",
        "    t_y = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n",
        "\n",
        "    # 5) Write back to global memory,\n",
        "    # transposing each element within the shared memory array.\n",
        "    transposed[t_y, t_x] = tile[cuda.threadIdx.x, cuda.threadIdx.y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxu31CAo0Mn7"
      },
      "source": [
        "### Check Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57KiZFZW0Mn7"
      },
      "source": [
        "Assuming you have correctly resolved the bank conflicts, this kernel should run significantly faster than both the naive transpose kernel, and, the shared memory (with bank conflicts) transpose kernel. In order to pass the assessment, your kernel will need to run on average in less than 840 ¬µs.\n",
        "\n",
        "The first value printed by running the following cell will give you the average run time of your kernel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "9HI3hGoS0Mn7"
      },
      "source": [
        "%timeit tile_transpose_conflict_free[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAVpVc2u0Mn9"
      },
      "source": [
        "## Summary"
      ]
    }
  ]
}