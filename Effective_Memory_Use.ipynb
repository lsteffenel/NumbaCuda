{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "Effective+Memory+Use.ipynb",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/NumbaCuda/blob/main/Effective_Memory_Use.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avant de commencer\n",
        "L'ex√©cution de ces notebooks sur Colab n√©cessite deux choses (au 4/2/2025) :\n",
        "\n",
        "1. des resources GPU\n",
        "  * Menu \"Ex√©cution\" -> \"Modifier le type d'ex√©cution\"\n",
        "2. D'utiliser une version plus ancienne de Colab en raison de certaines incompatibilit√©s du pilote Nvidia\n",
        "  * Connecter l'environnement d'ex√©cution\n",
        "  * Menu \"Outils\" -> \"Pallette de commandes\". Cherchez \"version\" dans la barre et s√©lectionnez l'option \"Utiliser la version d'environnement d'ex√©cution de remplacement\"\n"
      ],
      "metadata": {
        "id": "qcfzgaA2mW7U"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVtAcH6Q0Mna"
      },
      "source": [
        "# Utilisation efficace du sous-syst√®me de m√©moire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpyt3gzT0Mnb"
      },
      "source": [
        "Maintenant que vous savez √©crire des noyaux CUDA  et que vous comprenez l'importance de lancer des grilles pour donner suffisamment de travail au GPU afin de masquer la latence, vous allez apprendre des techniques pour utiliser efficacement la m√©moire du GPU. Ces techniques sont largement applicables √† une vari√©t√© d'applications CUDA et sont parmi les plus importantes lorsqu'il s'agit d'acc√©l√©rer votre code CUDA.\n",
        "\n",
        "Vous allez commencer par en apprendre davantage sur la coalescence de m√©moire (regroupement/organisation de blocs m√©moire). Pour tester votre capacit√© √† raisonner sur la coalescence, vous d√©couvrirez ensuite les grilles bidimensionnelles et les blocs de threads. Ensuite, vous d√©couvrirez comment utiliser la m√©moire partag√©e, qui sera utilis√©e pour faciliter la coalescence l√† o√π cela n'aurait pas √©t√© possible autrement. Enfin, vous d√©couvrirez les conflits qui peuvent arriver avec la m√©moire partag√©e et une technique pour les r√©soudre.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3GIL0le0Mnd"
      },
      "source": [
        "## Le probl√®me : l'acc√®s \"√©parpill√©\" √† la m√©moire nuit les performances"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "loksj-jY0Mnd"
      },
      "source": [
        "Avant d‚Äôapprendre les d√©tails sur la coalescence, ex√©cutez les cellules suivantes pour observer les implications en termes de performances d‚Äôun changement apparemment trivial du mode d‚Äôacc√®s aux donn√©es."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--TayTTy0Mnd"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rMOxcJrS0Mne"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DchcEDY10Mng"
      },
      "source": [
        "### Data Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbUfmnzj0Mng"
      },
      "source": [
        "Dans cette cellule, nous d√©finissons `n` et cr√©ons une grille avec `n` threads. Nous cr√©ons √©galement un vecteur de sortie de longueur `n`. Pour les entr√©es, nous cr√©ons des vecteurs de taille `stride * n` pour des raisons qui seront expliqu√©es ci-dessous :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "sSIYp-Rl0Mnh"
      },
      "source": [
        "n = 1024*1024 # 1M\n",
        "\n",
        "threads_per_block = 1024\n",
        "blocks = int(n / threads_per_block)\n",
        "\n",
        "stride = 16\n",
        "\n",
        "# Input Vectors of length stride * n\n",
        "a = np.ones(stride * n).astype(np.float32)\n",
        "b = a.copy().astype(np.float32)\n",
        "\n",
        "# Output Vector\n",
        "out = np.zeros(n).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_b = cuda.to_device(b)\n",
        "d_out = cuda.to_device(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuktrimW0Mnh"
      },
      "source": [
        "### Kernel Definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0JmJ9l60Mnh"
      },
      "source": [
        "Dans `add_experiment`, chaque thread de la grille ajoutera un √©l√©ment dans `a` et un √©l√©ment dans `b` puis √©crira le r√©sultat dans `out`. Le noyau a √©t√© √©crit de telle sorte que nous puissions passer une valeur `coalesced` de `True` ou `False` pour affecter la fa√ßon dont il indexe dans les vecteurs `a` et `b`. Vous verrez la comparaison des performances des deux modes ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "J1pNj0RP0Mni"
      },
      "source": [
        "@cuda.jit\n",
        "def add_experiment(a, b, out, stride, coalesced):\n",
        "    i = cuda.grid(1)\n",
        "    # The above line is equivalent to\n",
        "    # i = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    if coalesced == True:\n",
        "        out[i] = a[i] + b[i]\n",
        "    else:\n",
        "        out[i] = a[stride*i] + b[stride*i]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tov38eY0Mni"
      },
      "source": [
        "### Lancement d'un kernet avec un acc√®s \"coalesced\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EzN1lvF0Mnj"
      },
      "source": [
        "Ici, nous passons ¬´ True ¬ª comme valeur ¬´ coalesced ¬ª et observons les performances du noyau sur plusieurs ex√©cutions¬†:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "qP_h6zzm0Mnj"
      },
      "source": [
        "%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, stride, True); cuda.synchronize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94KnxXDH0Mnj"
      },
      "source": [
        "V√©rifions si le noyau s'ex√©cute comme attendu :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "E7ilWl6D0Mnk"
      },
      "source": [
        "result = d_out.copy_to_host()\n",
        "truth = a[:n] + b[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "9U02BdKP0Mnk"
      },
      "source": [
        "np.array_equal(result, truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkMjgEWq0Mnk"
      },
      "source": [
        "### Lancement d'un noyau sans acc√®s coalescent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYiuF8ZR0Mnk"
      },
      "source": [
        "Dans cette cellule, nous passons \"¬†False¬†\" pour observer les performances du mod√®le d'acc√®s aux donn√©es non coalescents¬†:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HY9oGSDt0Mnl"
      },
      "source": [
        "%timeit add_experiment[blocks, threads_per_block](d_a, d_b, d_out, stride, False); cuda.synchronize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VodDON60Mnl"
      },
      "source": [
        "V√©rifions si le noyau s'ex√©cute comme attendu :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "e4yAsZOD0Mnl"
      },
      "source": [
        "result = d_out.copy_to_host()\n",
        "truth = a[::stride] + b[::stride]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "UgxF3vkt0Mnl"
      },
      "source": [
        "np.array_equal(result, truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6K4dg590Mnl"
      },
      "source": [
        "### R√©sultats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVKtxTET0Mnm"
      },
      "source": [
        "Les performances du mode d'acc√®s \"non coalescent\" sont bien pires. Vous allez maintenant d√©couvrir pourquoi et comment r√©fl√©chir aux modes d'acc√®s aux donn√©es pour obtenir des noyaux tr√®s performants."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Y7yB1vg0Mnm"
      },
      "source": [
        "## Pr√©sentation : Global Memory Coalescing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV0LZIyR0Mnm"
      },
      "source": [
        "Regardez la pr√©sentation ci-dessous :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7PYWV1wY0Mnm"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/coalescing-v3.pptx', 800, 450)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEVtOBbR0Mnm"
      },
      "source": [
        "## Exercice : Somme des Colonnes et Lignes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBhLKf6u0Mnn"
      },
      "source": [
        "Pour cet exercice, il vous sera demand√© d'√©crire un noyau pour faire la somme des colonnes, utilisant le mode d'acc√®s m√©moire coalesc√©s. Pour commencer, vous observerez les performances sans ce mode d' acc√®s m√©moire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oa9jRAJI0Mnn"
      },
      "source": [
        "### Somme des lignes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m-bKCcyD0Mnn"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "yIVrXcGn0Mnn"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDlkLI2p0Mnn"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQL4dpWq0Mnn"
      },
      "source": [
        "Dans ce paragraphe nous cr√©ons une matrice pour l'entr√©e ainsi qu'un vecteur pour stocker la solution, et nous transf√©rons chacun d'eux vers le p√©riph√©rique. Nous d√©finissons √©galement les dimensions de la grille et du bloc √† utiliser lorsque nous lan√ßons le noyau.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Sou7fNbf0Mno"
      },
      "source": [
        "n = 16384 # matrix side size\n",
        "threads_per_block = 256\n",
        "blocks = int(n / threads_per_block)\n",
        "\n",
        "# Input Matrix\n",
        "a = np.ones(n*n).reshape(n, n).astype(np.float32)\n",
        "# Here we set an arbitrary row to an arbitrary value to facilitate a check for correctness below.\n",
        "a[3] = 9\n",
        "\n",
        "# Output vector\n",
        "sums = np.zeros(n).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_sums = cuda.to_device(sums)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQYPTiQT0Mno"
      },
      "source": [
        "**Le noyau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xibILyND0Mno"
      },
      "source": [
        "`row_sums` utilisera chaque thread pour parcourir une ligne de donn√©es, effectuer la somme, puis stockera la somme des lignes dans `sums`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "67woR5pO0Mno"
      },
      "source": [
        "@cuda.jit\n",
        "def row_sums(a, sums, n):\n",
        "    idx = cuda.grid(1)\n",
        "    sum = 0.0\n",
        "\n",
        "    for i in range(n):\n",
        "        # Each thread will sum a row of `a`\n",
        "        sum += a[idx][i]\n",
        "\n",
        "    sums[idx] = sum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee57AO4_0Mno"
      },
      "source": [
        "**Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "TZS_9bfZ0Mno"
      },
      "source": [
        "%timeit row_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzLicTRw0Mnp"
      },
      "source": [
        "**V√©rification du r√©sultat**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rUsKLQMi0Mnp"
      },
      "source": [
        "result = d_sums.copy_to_host()\n",
        "truth = a.sum(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cajPu6xc0Mnp"
      },
      "source": [
        "np.array_equal(truth, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ofbySFG0Mnp"
      },
      "source": [
        "### Somme des colonnes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OckAKRIG0Mnp"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "HohrB1RC0Mnp"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJpUHYgy0Mnp"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eNsBt1w0Mnq"
      },
      "source": [
        "On reprend le m√™me format pr√©c√©dent, mais avec des valeurs sur les colonnes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cemu8-A-0Mnq"
      },
      "source": [
        "n = 16384 # matrix side size\n",
        "threads_per_block = 256\n",
        "blocks = int(n / threads_per_block)\n",
        "\n",
        "a = np.ones(n*n).reshape(n, n).astype(np.float32)\n",
        "# Here we set an arbitrary column to an arbitrary value to facilitate a check for correctness below.\n",
        "a[:, 3] = 9\n",
        "sums = np.zeros(n).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_sums = cuda.to_device(sums)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vQMYSju0Mnq"
      },
      "source": [
        "**D√©finition du noyau**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXuf1JVr0Mnq"
      },
      "source": [
        "`col_sums` utilisera chaque thread pour parcourir une colonne de donn√©es, en la sommant, puis stockera la somme de sa colonne dans `sums`. Compl√©tez la d√©finition du noyau pour y parvenir (c'est √† vous de le faire üòÄ)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vSBaE_qy0Mnq"
      },
      "source": [
        "@cuda.jit\n",
        "def col_sums(a, sums, ds):\n",
        "    # TODO: Write this kernel to store the sum of each column in matrix `a` to the `sums` vector.\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i15Fke5q0Mnq"
      },
      "source": [
        "**V√©rification de la Performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQgYMZx30Mnr"
      },
      "source": [
        "En supposant que vous ayez √©crit `col_sums` pour utiliser l'acc√®s coalescent, vous devriez voir une acc√©l√©ration significative (presque 2x) par rapport aux `row_sums` \"non coalescent\" que vous avez ex√©cut√©s ci-dessus¬†:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "f35hHNfm0Mnr"
      },
      "source": [
        "%timeit col_sums[blocks, threads_per_block](d_a, d_sums, n); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfgNd_IL0Mnr"
      },
      "source": [
        "**V√©rification des r√©sultats**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIYJS3FY0Mnr"
      },
      "source": [
        "Confirm your kernel is working as expected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "y9qXj5Kl0Mnr"
      },
      "source": [
        "result = d_sums.copy_to_host()\n",
        "truth = a.sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "7ytHoxap0Mnr"
      },
      "source": [
        "np.array_equal(truth, result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cY-RQRW10Mnr"
      },
      "source": [
        "## Des blocs et grilles √† 2 et 3 dimensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9bCiskf0Mns"
      },
      "source": [
        "Les grilles et les blocs peuvent √™tre configur√©s pour contenir respectivement une collection bidimensionnelle ou tridimensionnelle de blocs ou de threads. Cela est fait principalement pour des raisons de commodit√© pour les programmeurs qui travaillent avec des donn√©es bidimensionnels ou tridimensionnels. Voici un exemple tr√®s simple pour mettre en √©vidence la syntaxe. Il faudra comprendre la d√©finition du noyau et comme il est lanc√© pour que le concept n'ait un sens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "MoYug8R90Mns"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rH9lgSHA0Mns"
      },
      "source": [
        "A = np.zeros((4,4)) # A 4x4 Matrix of 0's\n",
        "d_A = cuda.to_device(A)\n",
        "\n",
        "# Here we create a 2D grid with 4 blocks in a 2x2 structure, each with 4 threads in a 2x2 structure\n",
        "# by using a Python tuple to signify grid and block dimensions.\n",
        "blocks = (2, 2)\n",
        "threads_per_block = (2, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4kDlvZo0Mns"
      },
      "source": [
        "Ce noyau prendra une matrice d'entr√©e de 0 et √©crira chacun de ses √©l√©ments directement dans la grille au format `X.Y`` :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "IxghwLch0Mns"
      },
      "source": [
        "@cuda.jit\n",
        "def get_2D_indices(A):\n",
        "    # By passing `2`, we get the thread's unique x and y coordinates in the 2D grid\n",
        "    x, y = cuda.grid(2)\n",
        "    # The above is equivalent to the following 2 lines of code:\n",
        "    # x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    # y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
        "\n",
        "    # Write the x index followed by a decimal and the y index.\n",
        "    A[x][y] = x + y / 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "RZtWAc4c0Mns"
      },
      "source": [
        "get_2D_indices[blocks, threads_per_block](d_A)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "3Dytd6qd0Mnt"
      },
      "source": [
        "result = d_A.copy_to_host()\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNPnk2YL0Mnt"
      },
      "source": [
        "## Exercice : Somme de matrices 2D en mode coalescent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UytTSf790Mnt"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "SatpVW8U0Mnt"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CcT0xG90Mnt"
      },
      "source": [
        "### Data Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVciXU660Mnt"
      },
      "source": [
        "Dans cette cellule, nous d√©finissons des matrices d'entr√©e d'√©l√©ments 2048x2048 `a` et `b`, ainsi qu'une matrice de sortie initialis√©e de 2048x2048. Nous copions ces matrices sur le GPU.\n",
        "\n",
        "Nous d√©finissons √©galement les dimensions de bloc et de grille √† 2 dimensions. Notez que nous cr√©ons une grille avec le m√™me nombre total de threads que d'√©l√©ments d'entr√©e et de sortie, de sorte que chaque thread de la grille calculera la somme pour un seul √©l√©ment de la matrice de sortie."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "q_TLDr1Z0Mnu"
      },
      "source": [
        "n = 2048*2048 # 4M\n",
        "\n",
        "# 2D blocks\n",
        "threads_per_block = (32, 32)\n",
        "# 2D grid\n",
        "blocks = (64, 64)\n",
        "\n",
        "# 2048x2048 input matrices\n",
        "a = np.arange(n).reshape(2048,2048).astype(np.float32)\n",
        "b = a.copy().astype(np.float32)\n",
        "\n",
        "# 2048x2048 0-initialized output matrix\n",
        "out = np.zeros_like(a).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_b = cuda.to_device(b)\n",
        "d_out = cuda.to_device(out)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eFw5ZJu0Mnu"
      },
      "source": [
        "### Somme pour une matrice 2D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l05Yos500Mnu"
      },
      "source": [
        "Votre t√¢che consiste √† compl√©ter les t√¢ches √† effectuer dans `matrix_add` pour additionner correctement `a` et `b` dans `out`. Pour vous aider √† comprendre les modes d'acc√®s, `matrix_add` acceptera un bool√©en `coalesced` indiquant si les mod√®les d'acc√®s doivent √™tre coalescents ou non. Les deux modes (coalesced et uncoalesced) devraient produire des r√©sultats corrects, cependant, vous devriez observer des acc√©l√©rations significatives ci-dessous lors de l'ex√©cution avec `coalesced` d√©fini sur `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "V4uR7ezk0Mnu"
      },
      "source": [
        "@cuda.jit\n",
        "def matrix_add(a, b, out, coalesced):\n",
        "    # TODO: set x and y to index correctly such that each thread\n",
        "    # accesses one element in the data.\n",
        "    x, y = pass\n",
        "\n",
        "    if coalesced == True:\n",
        "        # TODO: write the sum of one element in `a` and `b` to `out`\n",
        "        # using a coalesced memory access pattern.\n",
        "    else:\n",
        "        # TODO: write the sum of one element in `a` and `b` to `out`\n",
        "        # using an uncoalesced memory access pattern."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMzVBdpm0Mnv"
      },
      "source": [
        "### V√©rification de la performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wEdq7G50Mnv"
      },
      "source": [
        "Ex√©cutez les deux cellules ci-dessous pour lancer `matrix_add` avec les mod√®les d'acc√®s que vous avez √©crits, et observez la diff√©rence de performances. Des cellules suppl√©mentaires ont √©t√© fournies pour confirmer l'exactitude de votre noyau."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMLqgTX30Mnv"
      },
      "source": [
        "**Coalesced**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "OpvsdKTy0Mnv"
      },
      "source": [
        "%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, True); cuda.synchronize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "Z0WO3j2F0Mnw"
      },
      "source": [
        "result = d_out.copy_to_host()\n",
        "truth = a+b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "smvzVIio0Mnw"
      },
      "source": [
        "np.array_equal(result, truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPRV7EZR0Mnw"
      },
      "source": [
        "**Uncoalesced**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "tsCvAmBI0Mnw"
      },
      "source": [
        "%timeit matrix_add[blocks, threads_per_block](d_a, d_b, d_out, False); cuda.synchronize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GaEAgTi_0Mnx"
      },
      "source": [
        "result = d_out.copy_to_host()\n",
        "truth = a+b"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "J9SAA83P0Mnx"
      },
      "source": [
        "np.array_equal(result, truth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwXlyrZT0Mnx"
      },
      "source": [
        "## M√©moire Partag√©e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dse3CL4a0Mnx"
      },
      "source": [
        "Jusqu'√† pr√©sent, nous avons fait la distinction entre la m√©moire de l'h√¥te et celle de la GPU, comme si la m√©moire GPU √©tait un seul type de m√©moire. Mais en fait, CUDA a une [hi√©rarchie de m√©moire](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#memory-hierarchy) encore plus fine. La m√©moire du dispositif que nous avons utilis√©e jusqu'√† pr√©sent est appel√©e **m√©moire globale**, disponible pour n'importe quel thread ou bloc sur l'appareil, et qui peut persister pendant toute la dur√©e de vie de l'application. Naturellement, c'est un espace m√©moire relativement grand.\n",
        "\n",
        "Nous allons maintenant discuter de la mani√®re d'utiliser une r√©gion de la m√©moire appel√©e **m√©moire partag√©e**. La m√©moire partag√©e est un cache d√©fini par le programmeur, et de taille limit√©e qui [d√©pend du GPU](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities). Elle est **partag√©e** entre tous les threads d'un bloc. Il s'agit d'une ressource rare, √† laquelle les threads ext√©rieurs au bloc ne peuvent pas acc√©der et qui ne persiste pas apr√®s la fin de l'ex√©cution d'un noyau. La m√©moire partag√©e a cependant une bande passante beaucoup plus √©lev√©e que la m√©moire globale et peut √™tre utilis√©e avec plus d'efficacit√© dans de nombreux noyaux, en particulier pour optimiser les performances.\n",
        "\n",
        "Voici quelques cas d'utilisation courants de la m√©moire partag√©e¬†:\n",
        "\n",
        "* Mise en cache de la m√©moire lue √† partir de la m√©moire globale qui devra √™tre lue plusieurs fois dans un bloc.\n",
        "* Mise en m√©moire tampon de la sortie des threads afin qu'elle puisse √™tre fusionn√©e (coalescence) avant de la r√©√©crire dans la m√©moire globale.\n",
        "* Stockage temporaire de donn√©es utilis√©s dans des op√©rations gather/scatter dans un bloc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44wmMLl40Mnx"
      },
      "source": [
        "### La syntaxe ppour la m√©moire partag√©e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-rvCoReK0Mnx"
      },
      "source": [
        "Numba fournit des [fonctions](https://numba.pydata.org/numba-doc/dev/cuda/memory.html#shared-memory-and-thread-synchronization) pour l'allocation de m√©moire partag√©e ainsi que pour la synchronisation entre les threads d'un bloc, ce qui est souvent n√©cessaire apr√®s que des threads parall√®les ont lu ou √©crit dans la m√©moire partag√©e.\n",
        "\n",
        "Lorsque vous d√©clarez une m√©moire partag√©e, vous fournissez la forme du tableau partag√©, ainsi que son type, √† l'aide d'un [type Numba](https://numba.pydata.org/numba-doc/dev/reference/types.html#numba-types). **La forme du tableau doit √™tre une valeur constante**, et par cons√©quent, vous ne pouvez pas utiliser d'arguments pass√©s √† la fonction ni des variables fournies comme `numba.cuda.blockDim.x`, ou les valeurs calcul√©es de `cuda.griddim`.\n",
        "\n",
        "Comme ce concepte est un peu plus compliqu√©, voici un exemple pour d√©montrer la syntaxe avec des commentaires soulignant le mouvement de la m√©moire h√¥te vers la m√©moire globale du dispositif, vers la m√©moire partag√©e, vers la m√©moire globale du p√©riph√©rique et enfin vers la m√©moire h√¥te¬†:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhH49D360Mny"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVFzMbDi0Mny"
      },
      "source": [
        "Nous utiliserons `numba.types` pour d√©finir les types de valeurs dans la m√©moire partag√©e."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "8CpCtKmq0Mny"
      },
      "source": [
        "import numpy as np\n",
        "from numba import types, cuda"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjLTtBxd0Mny"
      },
      "source": [
        "**<Echange (swap) d'√©l√©ments avec la m√©moire partag√©e**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VENFAVZZ0Mny"
      },
      "source": [
        "Le noyau suivant prend un vecteur d'entr√©e, o√π chaque thread √©crira d'abord un √©l√©ment dans la m√©moire partag√©e puis, apr√®s avoir synchronis√© de telle sorte que tous les √©l√©ments aient √©t√© √©crits dans la m√©moire partag√©e, √©crira un √©l√©ment de la m√©moire partag√©e dans le vecteur de sortie.\n",
        "\n",
        "Il convient de noter que chaque thread √©crira une valeur √† partir d'une position de la m√©moire partag√©e, laquelle a √©t√© √©crite par un autre thread."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xko3UNwG0Mny"
      },
      "source": [
        "@cuda.jit\n",
        "def swap_with_shared(vector, swapped):\n",
        "    # Allocate a 4 element vector containing int32 values in shared memory.\n",
        "    temp = cuda.shared.array(4, dtype=types.int32)\n",
        "\n",
        "    idx = cuda.grid(1)\n",
        "\n",
        "    # Move an element from global memory into shared memory\n",
        "    temp[idx] = vector[idx]\n",
        "\n",
        "    # cuda.syncthreads will force all threads in the block to synchronize here, which is necessary because...\n",
        "    cuda.syncthreads()\n",
        "    #...the following operation is reading an element written to shared memory by another thread.\n",
        "\n",
        "    # Move an element from shared memory back into global memory\n",
        "    swapped[idx] = temp[3 - cuda.threadIdx.x] # swap elements"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ-9_3ow0Mny"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "7Mr2lOMY0Mnz"
      },
      "source": [
        "vector = np.arange(4).astype(np.int32)\n",
        "swapped = np.zeros_like(vector)\n",
        "\n",
        "# Move host memory to device (global) memory\n",
        "d_vector = cuda.to_device(vector)\n",
        "d_swapped = cuda.to_device(swapped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "6aLXu-JS0Mnz"
      },
      "source": [
        "vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DPD7p3S0Mnz"
      },
      "source": [
        "**Ex√©cution du Kernel**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "FxwDr5M70Mnz"
      },
      "source": [
        "swap_with_shared[1, 4](d_vector, d_swapped)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "St56M2zB0Mnz"
      },
      "source": [
        "**V√©rifier les R√©sultats**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "dpICY2U80Mnz"
      },
      "source": [
        "# Move device (global) memory back to the host\n",
        "result = d_swapped.copy_to_host()\n",
        "result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S8GEvKz0Mnz"
      },
      "source": [
        "## Pr√©sentation : M√©moire partag√©e en mode coalescence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKdZZRjl0Mn0"
      },
      "source": [
        "Ex√©cutez la cellule suivante pour charger les diapositives, puis cliquez sur ¬´¬†D√©marrer le diaporama¬†¬ª pour les mettre en plein √©cran."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "JNpWnV4L0Mn0"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/shared_coalescing.pptx', 800, 450)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gT6yvrB0Mn0"
      },
      "source": [
        "## Exercice¬†: Utilisation de la m√©moire partag√©e pour les lectures et √©critures fusionn√©es avec transposition de matrice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_uoQDSk0Mn0"
      },
      "source": [
        "Dans cet exercice, vous allez mettre en ≈ìuvre ce qui vient d'√™tre d√©montr√© dans la pr√©sentation en √©crivant un noyau pour la transposition d'une matrice qui, en utilisant la m√©moire partag√©e, effectue des lectures et des √©critures coalesc√©es dans la matrice de sortie localis√©e dans la m√©moire globale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5Nar1Os0Mn0"
      },
      "source": [
        "### Lectures coalescentes, √©critures non-coalescentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_Zzf6sE0Mn0"
      },
      "source": [
        "√Ä titre de comparaison des performances, voici un noyau de transposition de matrice na√Øf qui effectue des lectures coalesc√©es √† partir de l'entr√©e, mais des √©critures non coalesc√©es vers la sortie."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOZH4ME10Mn0"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "GxUeJafe0Mn1"
      },
      "source": [
        "from numba import cuda\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2qqSTzg0Mn1"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6I8hVQ-0Mn1"
      },
      "source": [
        "Ici, nous cr√©ons une matrice d'entr√©e 4096x4096 `a` ainsi qu'une matrice de sortie 4096x4096 `transpos√©e`, et les copions sur l'appareil.\n",
        "\n",
        "Nous d√©finissons √©galement une grille bidimensionnelle avec des blocs bidimensionnels √† utiliser ci-dessous. Notez que nous avons cr√©√© une grille avec un nombre total de threads √©gal au nombre d'√©l√©ments dans la matrice d'entr√©e."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "A4dvUTyq0Mn1"
      },
      "source": [
        "n = 4096*4096 # 16M\n",
        "\n",
        "# 2D blocks\n",
        "threads_per_block = (32, 32)\n",
        "#2D grid\n",
        "blocks = (128, 128)\n",
        "\n",
        "# 4096x4096 input and output matrices\n",
        "a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n",
        "transposed = np.zeros_like(a).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_transposed = cuda.to_device(transposed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfcahheb0Mn1"
      },
      "source": [
        "**Naive Matrix Transpose Kernel**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imgIJyzh0Mn1"
      },
      "source": [
        "Ce noyau transpose correctement ¬´ a ¬ª, en √©crivant la transposition dans ¬´ transposed ¬ª. Il effectue des lectures depuis ¬´ a ¬ª de mani√®re coalesc√©e, cependant, ses √©critures dans ¬´ transposed ¬ª ne sont pas coalesc√©es."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "T7USDX080Mn1"
      },
      "source": [
        "@cuda.jit\n",
        "def transpose(a, transposed):\n",
        "    x, y = cuda.grid(2)\n",
        "\n",
        "    transposed[x][y] = a[y][x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EE6C-mU50Mn2"
      },
      "source": [
        "**Check Performance**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "bN28INj20Mn2"
      },
      "source": [
        "%timeit transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyye0Iye0Mn2"
      },
      "source": [
        "**Check Correctness**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "gMhV_Yar0Mn2"
      },
      "source": [
        "result = d_transposed.copy_to_host()\n",
        "expected = a.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "pw_cFWke0Mn2"
      },
      "source": [
        "np.array_equal(result, expected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CxXi3jr0Mn2"
      },
      "source": [
        "### Exercice : r√©√©crire le code pour que les lectures et √©critures soient coalescentes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVQtAOPX0Mn2"
      },
      "source": [
        "Votre travail consistera √† refactoriser le noyau ¬´¬†transpose¬†¬ª pour utiliser la m√©moire partag√©e et effectuer √† la fois des lectures et des √©critures depuis la m√©moire globale de mani√®re fusionn√©e."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTiJpAky0Mn2"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "w0SlRnJA0Mn3"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda, types as numba_types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovwo4JMi0Mn3"
      },
      "source": [
        "**Data Creation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "WkHRO5Ko0Mn3"
      },
      "source": [
        "n = 4096*4096 # 16M\n",
        "\n",
        "# 2D blocks\n",
        "threads_per_block = (32, 32)\n",
        "#2D grid\n",
        "blocks = (128, 128)\n",
        "\n",
        "# 4096x4096 input and output matrices\n",
        "a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n",
        "transposed = np.zeros_like(a).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_transposed = cuda.to_device(transposed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxjIWxd70Mn3"
      },
      "source": [
        "**√âcrire un noyau de transposition qui utilise la m√©moire partag√©e**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSvTbmTZ0Mn3"
      },
      "source": [
        "Compl√©tez les `TODO` dans la d√©finition du noyau `tile_transpose`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "ts-KOdVT0Mn3"
      },
      "source": [
        "@cuda.jit\n",
        "def tile_transpose(a, transposed):\n",
        "    # `tile_transpose` assumes it is launched with a 32x32 block dimension,\n",
        "    # and that `a` is a multiple of these dimensions.\n",
        "\n",
        "    # 1) Create 32x32 shared memory array.\n",
        "\n",
        "    # TODO: Your code here.\n",
        "\n",
        "    # Compute offsets into global input array. Recall for coalesced access we want to map threadIdx.x increments to\n",
        "    # the fastest changing index in the data, i.e. the column in our array.\n",
        "    # Note: `a_col` and `a_row` are already correct.\n",
        "    a_col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    a_row = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
        "\n",
        "    # 2) Make coalesced read from global memory (using grid indices)\n",
        "    # into shared memory array (using thread indices).\n",
        "\n",
        "    # TODO: Your code here.\n",
        "\n",
        "    # 3) Wait for all threads in the block to finish updating shared memory.\n",
        "\n",
        "    # TODO: Your code here.\n",
        "\n",
        "    # 4) Calculate transposed location for the shared memory array tile\n",
        "    # to be written back to global memory. Note that blockIdx.y*blockDim.y\n",
        "    # and blockIdx.x* blockDim.x are swapped (because we want to write to the\n",
        "    # transpose locations), but we want to keep access coalesced, so match up the\n",
        "    # threadIdx.x to the fastest changing index, i.e. the column./\n",
        "    # Note: `t_col` and `t_row` are already correct.\n",
        "    t_col = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n",
        "    t_row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n",
        "\n",
        "    # 5) Write from shared memory (using thread indices)\n",
        "    # back to global memory (using grid indices)\n",
        "    # transposing each element within the shared memory array.\n",
        "\n",
        "    # TODO: Your code here."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlIfrROr0Mn4"
      },
      "source": [
        "**Check Performance**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8CNWMlg0Mn4"
      },
      "source": [
        "V√©rifiez les performances de votre noyau de transposition refactoris√©. Vous devriez constater une acc√©l√©ration par rapport aux performances de transposition de base ci-dessus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "I-dA5i0X0Mn4"
      },
      "source": [
        "%timeit tile_transpose[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqv47IKI0Mn4"
      },
      "source": [
        "**Check Correctness**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "1GsR6Y_u0Mn4"
      },
      "source": [
        "result = d_transposed.copy_to_host()\n",
        "expected = a.T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "VnbEeyFH0Mn4"
      },
      "source": [
        "np.array_equal(result, expected)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InCDnwgs0Mn4"
      },
      "source": [
        "### Pourquoi si peu de gains ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlfwNhul0Mn4"
      },
      "source": [
        "Bien qu'il s'agisse d'une acc√©l√©ration significative pour seulement quelques lignes de code, vous pourriez penser que l'am√©lioration des performances n'est pas aussi marqu√©e que vous l'esp√©riez par rapport aux exp√©riences pr√©c√©dentes. Il y a deux raisons principales √† cela¬†:\n",
        "\n",
        "1. Le noyau de transposition na√Øf effectuait des lectures fusionn√©es, donc votre version refactoris√©e n'a optimis√© que la moiti√© de l'acc√®s √† la m√©moire globale tout au long de l'ex√©cution du noyau.\n",
        "2. Votre code tel qu'il est √©crit souffre de conflits de m√©moire partag√©e, un sujet sur lequel nous allons maintenant porter notre attention.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42jEkFzX0Mn5"
      },
      "source": [
        "## Pr√©sentation : Conflits de m√©moire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3bV4pX90Mn5"
      },
      "source": [
        "Ex√©cutez la cellule suivante pour charger les diapositives, puis cliquez sur ¬´¬†D√©marrer le diaporama¬†¬ª pour les mettre en plein √©cran."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "rA-Fd-8Z0Mn5"
      },
      "source": [
        "from IPython.display import IFrame\n",
        "IFrame('https://view.officeapps.live.com/op/view.aspx?src=https://developer.download.nvidia.com/training/courses/C-AC-02-V1/bank_conflicts.pptx', 800, 450)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJL6U1Mf0Mn5"
      },
      "source": [
        "## Exercice : R√©sourdre les conflits de m√©moire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be8-quUE0Mn5"
      },
      "source": [
        "En guise d'exercice final, vous refactoriserez le noyau de transposition en utilisant la m√©moire partag√©e pour qu'il soit exempt de conflit de banque de m√©moire partag√©e."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH45i_aH0Mn6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "vFnPfMgt0Mn6"
      },
      "source": [
        "import numpy as np\n",
        "from numba import cuda, types as numba_types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I1x8xjYU0Mn6"
      },
      "source": [
        "### Data Creation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "j2HJNvJt0Mn6"
      },
      "source": [
        "n = 4096*4096 # 16M\n",
        "threads_per_block = (32, 32)\n",
        "blocks = (128, 128)\n",
        "\n",
        "a = np.arange(n).reshape((4096,4096)).astype(np.float32)\n",
        "transposed = np.zeros_like(a).astype(np.float32)\n",
        "\n",
        "d_a = cuda.to_device(a)\n",
        "d_transposed = cuda.to_device(transposed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAz79gP_0Mn6"
      },
      "source": [
        "### Rendre le noyau sans conflit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jB2hn9kY0Mn6"
      },
      "source": [
        "Le noyau `tile_transpose_conflict_free` est un noyau de transposition de matrice qui utilise la m√©moire partag√©e de sorte que les lectures et les √©critures dans la m√©moire globale soient fusionn√©es. Votre travail consiste √† refactoriser le noyau afin qu'il ne souffre plus ces conflits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "cz-EYhxZ0Mn6"
      },
      "source": [
        "@cuda.jit\n",
        "def tile_transpose_conflict_free(a, transposed):\n",
        "    # `tile_transpose` assumes it is launched with a 32x32 block dimension,\n",
        "    # and that `a` is a multiple of these dimensions.\n",
        "\n",
        "    # 1) Create 32x32 shared memory array.\n",
        "    tile = cuda.shared.array((32, 32), numba_types.int32)\n",
        "\n",
        "    # Compute offsets into global input array.\n",
        "    x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
        "    y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
        "\n",
        "    # 2) Make coalesced read from global memory into shared memory array.\n",
        "    # Note the use of local thread indices for the shared memory write,\n",
        "    # and global offsets for global memory read.\n",
        "    tile[cuda.threadIdx.y, cuda.threadIdx.x] = a[y, x]\n",
        "\n",
        "    # 3) Wait for all threads in the block to finish updating shared memory.\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # 4) Calculate transposed location for the shared memory array tile\n",
        "    # to be written back to global memory.\n",
        "    t_x = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.x\n",
        "    t_y = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.y\n",
        "\n",
        "    # 5) Write back to global memory,\n",
        "    # transposing each element within the shared memory array.\n",
        "    transposed[t_y, t_x] = tile[cuda.threadIdx.x, cuda.threadIdx.y]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxu31CAo0Mn7"
      },
      "source": [
        "### Check Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57KiZFZW0Mn7"
      },
      "source": [
        "En supposant que vous ayez correctement r√©solu les conflits, ce noyau devrait s'ex√©cuter beaucoup plus rapidement que le noyau de transposition na√Øf et que le noyau de transposition √† m√©moire partag√©e (avec conflits). Pour r√©ussir l'√©valuation, votre noyau devra s'ex√©cuter en moyenne en moins de 840 ¬µs.\n",
        "\n",
        "La premi√®re valeur imprim√©e en ex√©cutant la cellule suivante vous donnera le temps d'ex√©cution moyen de votre noyau."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "9HI3hGoS0Mn7"
      },
      "source": [
        "%timeit tile_transpose_conflict_free[blocks, threads_per_block](d_a, d_transposed); cuda.synchronize()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}