{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lsteffenel/NumbaCuda/blob/main/Introduction_CUDA_Python_avec_Numba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Avant de commencer\n",
        "\n",
        "L'exécution de ces notebooks sur Colab nécessite deux choses (au 4/2/2025) :\n",
        "1. des resources GPU\n",
        "  * Menu \"Exécution\" -> \"Modifier le type d'exécution\"\n",
        "2. D'utiliser une version plus ancienne de Colab en raison de certaines incompatibilités du pilote Nvidia\n",
        "  * Menu \"Outils\" -> \"Pallette de commandes\". Cherchez \"version\" dans la barre et sélectionnez l'option \"Utiliser la version d'environnement d'exécution de remplacement\"\n",
        "\n"
      ],
      "metadata": {
        "id": "jdPw09Futu6y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYrO04vJtaf9"
      },
      "source": [
        "# Introduction à CUDA Python avec Numba\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkCWabrUtaf-"
      },
      "source": [
        "Dans cette première section, vous apprendrez d'abord à utiliser Numba pour compiler des fonctions pour la CPU, et recevrez une introduction au fonctionnement interne du compilateur Numba. Vous apprendrez ensuite comment accélérer par GPU les fonctions de tableau NumPy, ainsi que certaines techniques pour déplacer efficacement des données entre un hôte CPU et un périphérique GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L20BDniGtaf_"
      },
      "source": [
        "## C'est quoi Numba ?\n",
        "\n",
        "Numba est un compilateur de fonctions **just-in-time**, qui permet d'accélérer des codes numériques Python autant sur CPU que sur GPU :\n",
        "\n",
        " * **compilateur de fonction**: Numba compile des fonctions Python, au lieu de la totalité de l'application. Lors de l'exécution, Numba fait appel à ces fonctions compilées.\n",
        " * **compilateur typé**: Numba accélère les fonctions en les traduisant vers C ou Assembler. Pour cela, il nous permet d'indiquer le type des variables pour faire un code plus rapide.\n",
        " * **just-in-time**: Numba traduit et compile les functions lors de leur premier appel. La fonction compilée reste en cache pour des appels futurs.  \n",
        " * **orienté données numériques **: Numba est optimisz pour des données numériques telles que `int`, `float`, et `complex`.  Il y a très peu de support à des strings et probablement elles ne passent pas sur un GPU. La meilleure façon d'utiliser Numba est de s'appuier sur des arrays NumPy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daFjLMiBtaf_"
      },
      "source": [
        "## Premiers pas : Compiler pour le CPU\n",
        "\n",
        "Numba peut être utilisé pour optimiser le code pour un CPU ou un GPU. En guise d'introduction, écrivons notre première fonction Numba et compilons-la pour le **CPU**.\n",
        "\n",
        "Le compilateur Numba est généralement activé en appliquant un [**décorateur de fonction**](https://en.wikipedia.org/wiki/Python_syntax_and_semantics#Decorators) à une fonction Python. Les décorateurs sont des annotations qui transforment les fonctions Python qu'ils décorent, en utilisant une syntaxe très simple. Ici, nous utiliserons le décorateur de compilation CPU de Numba `@jit` :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "EZsGMTqitagA"
      },
      "outputs": [],
      "source": [
        "from numba import jit\n",
        "import math\n",
        "\n",
        "# This is the function decorator syntax and is equivalent to `hypot = jit(hypot)`.\n",
        "# The Numba compiler is just a function you can call whenever you want!\n",
        "@jit\n",
        "def hypot(x, y):\n",
        "    # Implementation from https://en.wikipedia.org/wiki/Hypot\n",
        "    x = abs(x);\n",
        "    y = abs(y);\n",
        "    t = min(x, y);\n",
        "    x = max(x, y);\n",
        "    t = t / x;\n",
        "    return x * math.sqrt(1+t*t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaoVhxkztagA"
      },
      "source": [
        "Essayons notre calcul d’hypoténuse :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "cOLaSek5tagA"
      },
      "outputs": [],
      "source": [
        "hypot(3.0, 4.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLvyHvI8tagA"
      },
      "source": [
        "Nous donnerons des détails plus tard, mais pour l'instant sachez que la première fois que nous appelons `hypot`, le compilateur est déclenché et compile une implémentation en code machine de la fonction pour les valeurs de type `float`. Numba enregistre également l'implémentation Python d'origine de la fonction dans l'attribut `.py_func`, afin que nous puissions appeler le code Python d'origine pour nous assurer d'obtenir la même réponse :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "AI0Xu_6YtagB"
      },
      "outputs": [],
      "source": [
        "hypot.py_func(3.0, 4.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jnq3AlB5tagB"
      },
      "source": [
        "### Benchmarking\n",
        "\n",
        "Une partie importante de l'utilisation de Numba consiste à mesurer les performances de votre nouveau code.\n",
        "\n",
        "Voyons si nous avons réellement accéléré quelque chose : le moyen le plus simple de le faire dans un bloc-notes Jupyter, comme celui dans lequel cette session est exécutée, est d'utiliser la [fonction magique `%timeit`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit).\n",
        "\n",
        "Commençons par mesurer la vitesse du Python d'origine :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "n6LPcwY9tagB"
      },
      "outputs": [],
      "source": [
        "%timeit hypot.py_func(3.0, 4.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-Pla6OItagB"
      },
      "source": [
        "La fonction `%timeit` exécute l'instruction plusieurs fois pour obtenir une estimation précise du temps d'exécution. Elle renvoie également le meilleur temps par défaut, ce qui est utile pour réduire la probabilité que des événements d'arrière-plan affectent votre mesure. L'approche du meilleur des 3 garantit également que le temps de compilation lors du premier appel ne fausse pas les résultats :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "85orcWpntagB"
      },
      "outputs": [],
      "source": [
        "%timeit hypot(3.0, 4.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7Rp0IKKtagB"
      },
      "source": [
        "Numba a fait du bon travail avec cette fonction. Elle est certainement plus rapide que la version Python pure. Bien sûr, la fonction `hypot` est déjà présente dans le module Python, voyons comment elle se compare :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "FgPPc-3BtagB"
      },
      "outputs": [],
      "source": [
        "%timeit math.hypot(3.0, 4.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QV1sFU22tagB"
      },
      "source": [
        "La fonction intégrée dans la bibliothèque math Python est encore plus rapide que Numba ! En effet, Numba introduit une surcharge à chaque appel de fonction qui est plus importante que la surcharge d'appel de fonction de Python lui-même.\n",
        "\n",
        "Les fonctions extrêmement rapides (comme celle ci-dessus) en souffriront."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc4bRUGOtagB"
      },
      "source": [
        "### Exercice : Utiliser Numba pour compiler une fonction pour le processeur\n",
        "\n",
        "La fonction suivante utilise [la méthode de Monte Carlo pour déterminer Pi](https://academo.org/demos/estimating-pi-monte-carlo/). La fonction elle-même fonctionne déjà, ne vous inquiétez donc pas des détails de l'implémentation mathématique.\n",
        "\n",
        "Complétez les deux `TODO` afin de compiler `monte_carlo_pi` avec Numba avant d'exécuter les 3 cellules suivantes qui :\n",
        "\n",
        "1. Confirment que la version compilée se comporte de la même manière que la version non compilée.\n",
        "2. Évaluent la version non compilée.\n",
        "3. Évaluent la version compilée."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "yVlIECMvtagB"
      },
      "outputs": [],
      "source": [
        "nsamples = 1000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_SrxPUT3tagB"
      },
      "outputs": [],
      "source": [
        "# TODO: Import Numba's just-in-time compiler function\n",
        "import random\n",
        "\n",
        "# TODO: Use the Numba compiler to compile this function\n",
        "def monte_carlo_pi(nsamples):\n",
        "    acc = 0\n",
        "    for i in range(nsamples):\n",
        "        x = random.random()\n",
        "        y = random.random()\n",
        "        if (x**2 + y**2) < 1.0:\n",
        "            acc += 1\n",
        "    return 4.0 * acc / nsamples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "i7zNwGWztagB"
      },
      "outputs": [],
      "source": [
        "# We will use numpy's `testing` library to confirm compiled and uncompiled versions run the same\n",
        "from numpy import testing\n",
        "\n",
        "# This assertion will fail until you successfully complete the exercise one cell above\n",
        "testing.assert_almost_equal(monte_carlo_pi(nsamples), monte_carlo_pi.py_func(nsamples), decimal=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6xsoCqtvtagC"
      },
      "outputs": [],
      "source": [
        "%timeit monte_carlo_pi(nsamples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "iAGYMDECtagC"
      },
      "outputs": [],
      "source": [
        "%timeit monte_carlo_pi.py_func(nsamples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1KbEDyZltagC"
      },
      "source": [
        "## Comment fonctionne Numba\n",
        "\n",
        "Maintenant que vous avez un peu mis les mains dans le cambouis avec le compilateur Numba, regardons ce qui se passe réellement sous le capot. La première fois que nous avons appelé notre fonction `hypot` enveloppée dans Numba, le compilateur traduit le code en langages assembleur de bas niveau (LLVM) et le compile. Au passage, il associe des types aux variables.\n",
        "\n",
        "Nous pouvons voir le résultat de l'inférence de type en utilisant la méthode `.inspect_types()`, qui imprime une version annotée du code source :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "ZaF39inTtagC"
      },
      "outputs": [],
      "source": [
        "hypot.inspect_types()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4Yb99pZtagC"
      },
      "source": [
        "Notez que les noms de type de Numba ont tendance à refléter [les noms de type NumPy](https://docs.scipy.org/doc/numpy-1.13.0/user/basics.types.html), donc un `float` Python est un `float64` (également appelé « double précision » dans d'autres langages). Il peut parfois être important de jeter un œil aux types de données dans le code GPU, car les performances des calculs `float32` et `float64` peuvent (selon le GPU) être très différentes sur les périphériques CUDA. Si votre algorithme peut obtenir des résultats corrects en utilisant `float32`, vous devriez probablement utiliser ce type de données, car le transtypage vers `float64` peut, selon le type de GPU, ralentir considérablement la fonction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5S0lsihLtagD"
      },
      "source": [
        "## Introduction à Numba pour le GPU avec les fonctions universelles NumPy (ufuncs)\n",
        "\n",
        "Nous commencerons notre présentation de la programmation GPU dans Numba en expliquant comment compiler [les fonctions universelles NumPy \\(ou ufuncs\\)](https://docs.scipy.org/doc/numpy-1.15.1/reference/ufuncs.html) pour le GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ba1lMSU_tagD"
      },
      "source": [
        "La chose la plus importante à savoir sur la programmation GPU lorsque nous commençons est que le matériel GPU est conçu pour le *parallélisme des données*. Le débit maximal est atteint lorsque le GPU calcule les mêmes opérations sur de nombreux éléments différents à la fois.\n",
        "\n",
        "Les fonctions universelles NumPy, qui effectuent la même opération sur chaque élément d'un tableau NumPy, sont naturellement parallèles aux données, elles sont donc naturellement adaptées à la programmation GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC0gvHWRtagD"
      },
      "source": [
        "## Les fonctions universelles NumPy (ufuncs)\n",
        "\n",
        "NumPy a le concept de fonctions universelles (« ufuncs »), qui sont des fonctions qui peuvent prendre des tableaux NumPy de dimensions variables, ou scalaires, et les exploiter élément par élément.\n",
        "\n",
        "À titre d'exemple, nous utiliserons l'ufunc `add` de NumPy pour démontrer le mécanisme ufunc de base :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jrYkLRwOtagE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "a = np.array([1, 2, 3, 4])\n",
        "b = np.array([10, 20, 30, 40])\n",
        "\n",
        "np.add(a, b) # Returns a new NumPy array resulting from adding every element in `a` to every element in `b`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBhf5XXmtagE"
      },
      "source": [
        "Ufuncs peut également combiner des scalaires avec des tableaux :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "MLTKWTYFtagE"
      },
      "outputs": [],
      "source": [
        "np.add(a, 100) # Returns a new NumPy array resulting from adding 100 to every element in `a`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hufaSZNstagE"
      },
      "source": [
        "Des tableaux de dimensions différentes, mais compatibles, peuvent également être combinés via une technique appelée [*broadcasting*](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.broadcasting.html).\n",
        "\n",
        "Le tableau de dimension inférieure sera répliqué pour correspondre à la dimensionnalité du tableau de dimension supérieure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "odkVwTBXtagE"
      },
      "outputs": [],
      "source": [
        "c = np.arange(4*4).reshape((4,4))\n",
        "print('c:', c)\n",
        "\n",
        "np.add(b, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIckawewtagE"
      },
      "source": [
        "## Faire des ufuncs pour le GPU\n",
        "\n",
        "Numba a la capacité de créer des ufuncs *compilés*, un processus généralement pas si simple impliquant du code C.\n",
        "\n",
        "Avec Numba, vous implémentez simplement une fonction scalaire à exécuter sur toutes les entrées, vous la décorez avec `@vectorize`, et Numba déterminera les règles de diffusion pour vous."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDN5MAvLtagE"
      },
      "source": [
        "Dans ce tout premier exemple, nous utiliserons le décorateur `@vectorize` pour compiler et optimiser un ufunc pour le **CPU**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "BmweeGHUtagE"
      },
      "outputs": [],
      "source": [
        "from numba import vectorize\n",
        "\n",
        "@vectorize\n",
        "def add_ten(num):\n",
        "    return num + 10 # This scalar operation will be performed on each element"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wspavV_GtagE"
      },
      "outputs": [],
      "source": [
        "nums = np.arange(10)\n",
        "add_ten(nums) # pass the whole array into the ufunc, it performs the operation on each element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvVTbekitagE"
      },
      "source": [
        "Nous générons un ufunc qui utilise CUDA sur le GPU avec en plus une **signature de type explicite** et la définition de l'attribut `target`. L'argument de signature décrit les types à utiliser à la fois pour les arguments ufuncs et la valeur de retour :\n",
        "```python\n",
        "'return_value_type(argument1_value_type, argument2_value_type, ...)'\n",
        "```\n",
        "\n",
        "Voici un exemple simple d'un ufunc qui sera compilé pour un périphérique GPU compatible CUDA. Il attend deux valeurs `int64` et renvoie également une valeur `int64` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "9otqmxCYtagE"
      },
      "outputs": [],
      "source": [
        "@vectorize(['int64(int64, int64)'], target='cuda') # Type signature and target are required for the GPU\n",
        "def add_ufunc(x, y):\n",
        "    return x + y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JUnt04IatagE"
      },
      "outputs": [],
      "source": [
        "add_ufunc(a, b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdOo7SIetagE"
      },
      "source": [
        "Pour un appel de fonction aussi simple, beaucoup de choses se sont produites !\n",
        "\n",
        "* Compilation d'un noyau CUDA pour exécuter l'opération ufunc en parallèle sur tous les éléments d'entrée.\n",
        "* Mémoire GPU allouée pour les entrées et la sortie.\n",
        "* Copie des données d'entrée sur le GPU.\n",
        "* Exécution du noyau CUDA (fonction GPU) avec les dimensions de noyau correctes compte tenu des tailles d'entrée.\n",
        "* Copie du résultat du GPU vers le CPU.\n",
        "* Renvoi du résultat sous forme de tableau NumPy sur l'hôte.\n",
        "\n",
        "Comparé à une implémentation en C, le code Numba est remarquablement plus concis.\n",
        "\n",
        "Vous vous demandez peut-être à quelle vitesse notre exemple simple est sur le GPU ? Voyons voir :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uNqUDPRMtagF"
      },
      "outputs": [],
      "source": [
        "%timeit np.add(b, c)   # NumPy on CPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W7UF45JitagF"
      },
      "outputs": [],
      "source": [
        "%timeit add_ufunc(b, c) # Numba on GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSN6c7dbtagF"
      },
      "source": [
        "Attends, le GPU est *beaucoup plus lent* que le CPU ?? Pour le moment, c'est normal, car nous avons (délibérément) mal utilisé le GPU de plusieurs manières dans cet exemple. La façon dont nous avons mal utilisé le GPU aidera à clarifier quels types de problèmes sont bien adaptés au calcul GPU et lesquels sont mieux traités par le CPU :\n",
        "\n",
        "* **Nos entrées sont trop petites** : le GPU atteint des performances grâce au parallélisme, en opérant sur des milliers de valeurs à la fois. Nos entrées de test n'ont que 4 et 16 entiers, respectivement. Nous avons besoin d'un tableau beaucoup plus grand pour même occuper le GPU.\n",
        "\n",
        "* **Notre calcul est trop simple** : l'envoi d'un calcul au GPU implique une charge assez importante par rapport à l'appel d'une fonction sur le CPU. Si notre calcul n'implique pas suffisamment d'opérations mathématiques (souvent appelées « intensité arithmétique »), le GPU passera la plupart de son temps à attendre que les données se déplacent.\n",
        "* **Nous copions les données vers et depuis le GPU** : le coût de la copie des données vers et depuis le GPU est souvent très important, alors il sera préférable d'exécuter plusieurs opérations GPU en séquence. Dans ces cas, il est logique d'envoyer des données au GPU et de les y conserver jusqu'à ce que tout notre traitement soit terminé.\n",
        "* **Nos types de données sont plus grands que nécessaire** : Notre exemple utilise `int64` alors que nous n'en avons probablement pas besoin. Le code scalaire utilisant des types de données 32 et 64 bits s'exécute essentiellement à la même vitesse sur le processeur, et pour les types entiers, la différence peut ne pas être drastique, mais les types de données à virgule flottante 64 bits peuvent avoir un coût de performance significatif sur le GPU, selon le type de GPU.\n",
        "\n",
        "L'arithmétique de base sur des flottants 64 bits peut être de 2x (architecture Pascal Tesla) à 24x (architecture Maxwell GeForce) plus lente que les flottants 32 bits. Si vous utilisez des GPU plus modernes (Volta, Turing, Ampere), cela pourrait être beaucoup moins préoccupant. NumPy utilise par défaut des types de données 64 bits lors de la création de tableaux, il est donc important de définir l'attribut [`dtype`](https://docs.scipy.org/doc/numpy-1.14.0/reference/arrays.dtypes.html) ou d'utiliser la méthode [`ndarray.astype()`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.ndarray.astype.html) pour sélectionner des types 32 bits lorsque vous en avez besoin.\n",
        "\n",
        "Compte tenu de ce qui précède, essayons un exemple plus rapide sur le GPU en effectuant une opération avec une intensité arithmétique beaucoup plus grande, sur une entrée beaucoup plus grande et en utilisant un type de données 32 bits.\n",
        "\n",
        "**Remarque :** tout le code NumPy ne fonctionnera pas sur le GPU et, comme dans l'exemple suivant, nous devrons utiliser les fonctions « pi » et « exp » de la bibliothèque « math » au lieu de celles de NumPy. Veuillez consulter [la documentation Numba](https://numba.pydata.org/numba-doc/latest/reference/numpysupported.html) pour une couverture complète de la prise en charge de NumPy sur le GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "72X1iiprtagF"
      },
      "outputs": [],
      "source": [
        "import math # Note that for the CUDA target, we need to use the scalar functions from the math module, not NumPy\n",
        "\n",
        "SQRT_2PI = np.float32((2*math.pi)**0.5)  # Precompute this constant as a float32.  Numba will inline it at compile time.\n",
        "\n",
        "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
        "def gaussian_pdf(x, mean, sigma):\n",
        "    '''Compute the value of a Gaussian probability density function at x with given mean and sigma.'''\n",
        "    return math.exp(-0.5 * ((x - mean) / sigma)**2) / (sigma * SQRT_2PI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w-jiEGbgtagF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Evaluate the Gaussian a million times!\n",
        "x = np.random.uniform(-3, 3, size=1000000).astype(np.float32)\n",
        "mean = np.float32(0.0)\n",
        "sigma = np.float32(1.0)\n",
        "\n",
        "# Quick test on a single element just to make sure it works\n",
        "gaussian_pdf(x[0], 0.0, 1.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "eNBa9v0otagF"
      },
      "outputs": [],
      "source": [
        "import scipy.stats # for definition of gaussian distribution, so we can compare CPU to GPU time\n",
        "norm_pdf = scipy.stats.norm\n",
        "%timeit norm_pdf.pdf(x, loc=mean, scale=sigma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "L86IS-JXtagF"
      },
      "outputs": [],
      "source": [
        "%timeit gaussian_pdf(x, mean, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw31PEqktagF"
      },
      "source": [
        "C'est une amélioration assez importante, même en incluant la surcharge de copie de toutes les données vers et depuis le GPU. Les Ufunc qui utilisent des fonctions spéciales (`exp`, `sin`, `cos`, etc.) sur de grands ensembles de données fonctionnent particulièrement bien sur le GPU.\n",
        "\n",
        "Pour terminer notre comparaison, définissons et chronométrons notre fonction `gaussian_pdf` lorsqu'elle est optimisée par Numba pour le **CPU** :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "rfPLvlHFtagF"
      },
      "outputs": [],
      "source": [
        "@vectorize\n",
        "def cpu_gaussian_pdf(x, mean, sigma):\n",
        "    '''Compute the value of a Gaussian probability density function at x with given mean and sigma.'''\n",
        "    return math.exp(-0.5 * ((x - mean) / sigma)**2) / (sigma * SQRT_2PI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NDobFakXtagF"
      },
      "outputs": [],
      "source": [
        "%timeit cpu_gaussian_pdf(x, mean, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ache0Y0htagF"
      },
      "source": [
        "C'est beaucoup plus rapide que la version CPU non compilée, mais beaucoup plus lent que celle accélérée par GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLxprh9itagF"
      },
      "source": [
        "## Fonctions de périphérique CUDA\n",
        "\n",
        "Les fonctions Ufunc sont vraiment fantastiques si et quand vous souhaitez effectuer des opérations élément par élément, ce qui est une tâche très courante. Il existe cependant un certain nombre de fonctions qui ne correspondent pas à cette description.\n",
        "\n",
        "Pour compiler des fonctions pour le GPU qui ne sont **pas** des fonctions vectorisées élément par élément, nous utilisons `numba.cuda.jit`.\n",
        "\n",
        "Dans la section suivante de ce cours, nous travaillons intensivement avec `numba.cuda.jit`, mais pour l'instant, montrons comment l'utiliser pour décorer une fonction d'assistance, à utiliser par un ufunc accéléré par GPU, afin que vous n'ayez pas à entasser toute votre logique dans une seule définition ufunc.\n",
        "\n",
        "Remarquez que `polar_to_cartesian` ci-dessous ne nécessite pas de signature de type, et aussi, qu'il lui est passé deux valeurs scalaires, contrairement aux ufuncs vectorisés que nous avons utilisés (et comme `polar_distance` ci-dessous) qui attendent des tableaux NumPy comme arguments.\n",
        "\n",
        "L'argument `device=True` indique que la fonction décorée ne peut être appelée **que** à partir d'une fonction exécutée sur le GPU, et non à partir du code hôte du CPU :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "S8GSGj_BtagF"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "\n",
        "@cuda.jit(device=True)\n",
        "def polar_to_cartesian(rho, theta):\n",
        "    x = rho * math.cos(theta)\n",
        "    y = rho * math.sin(theta)\n",
        "    return x, y\n",
        "\n",
        "@vectorize(['float32(float32, float32, float32, float32)'], target='cuda')\n",
        "def polar_distance(rho1, theta1, rho2, theta2):\n",
        "    x1, y1 = polar_to_cartesian(rho1, theta1) # We can use device functions inside our GPU ufuncs\n",
        "    x2, y2 = polar_to_cartesian(rho2, theta2)\n",
        "\n",
        "    return ((x1 - x2)**2 + (y1 - y2)**2)**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wShD4RSrtagF"
      },
      "outputs": [],
      "source": [
        "n = 1000000\n",
        "rho1 = np.random.uniform(0.5, 1.5, size=n).astype(np.float32)\n",
        "theta1 = np.random.uniform(-np.pi, np.pi, size=n).astype(np.float32)\n",
        "rho2 = np.random.uniform(0.5, 1.5, size=n).astype(np.float32)\n",
        "theta2 = np.random.uniform(-np.pi, np.pi, size=n).astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "QVTvmFaptagG"
      },
      "outputs": [],
      "source": [
        "polar_distance(rho1, theta1, rho2, theta2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W9zHS4nVtagG"
      },
      "source": [
        "Notez que le compilateur CUDA intègre de manière agressive les fonctions de l'appareil, il n'y a donc généralement pas de surcharge pour les appels de fonction. De même, le « tuple » renvoyé par `polar_to_cartesian` n'est pas réellement créé en tant qu'objet Python, mais représenté temporairement en tant que structure, qui est ensuite optimisée par le compilateur."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSJp4NjgtagG"
      },
      "source": [
        "## Instructions Python autorisées sur le GPU\n",
        "\n",
        "Par rapport à des opérations supportés par Numba sur le CPU (qui est déjà limité), Numba sur le GPU a encore plus de limitations. Les instructions Python pris en charge incluent :\n",
        "\n",
        "* `if`/`elif`/`else`\n",
        "* Boucles `while` et `for`\n",
        "* Opérateurs mathématiques de base\n",
        "* Fonctions sélectionnées des modules `math` et `cmath`\n",
        "* Tuples\n",
        "\n",
        "Voir [le manuel Numba](http://numba.pydata.org/numba-doc/latest/cuda/cudapysupported.html) pour plus de détails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyoJ4FmztagG"
      },
      "source": [
        "### Exercice : accélération d'une fonction par le GPU\n",
        "\n",
        "Nous allons accélérer une fonction de « suppression du zéro ». Une opération courante lorsque l'on travaille avec des formes d'onde consiste à forcer toutes les valeurs d'échantillon inférieures à une certaine magnitude absolue à être nulles, afin d'éliminer le bruit de faible amplitude. Créons quelques exemples de données :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "_JmJdmj9tagG"
      },
      "outputs": [],
      "source": [
        "# This allows us to plot right here in the notebook\n",
        "%matplotlib inline\n",
        "\n",
        "# Hacking up a noisy pulse train\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "n = 100000\n",
        "noise = np.random.normal(size=n) * 3\n",
        "pulses = np.maximum(np.sin(np.arange(n) / (n / 23)) - 0.3, 0.0)\n",
        "waveform = ((pulses * 300) + noise).astype(np.int16)\n",
        "plt.plot(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tmXxp1utagG"
      },
      "source": [
        "Décorez maintenant cette fonction `zero_suppress` pour l'exécuter en tant qu'ufunc vectorisé sur le périphérique CUDA."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "52jpmdMJtagG"
      },
      "outputs": [],
      "source": [
        "# TODO vectorize the function\n",
        "def zero_suppress(waveform_value, threshold):\n",
        "    if waveform_value < threshold:\n",
        "        result = 0\n",
        "    else:\n",
        "        result = waveform_value\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kGQyExV1tagG"
      },
      "outputs": [],
      "source": [
        "# This will throw an error until you successfully vectorize the `zero_suppress` function above.\n",
        "# The noise on the baseline should disappear when zero_suppress is implemented\n",
        "plt.plot(zero_suppress(waveform, 15))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0T90listagG"
      },
      "source": [
        "## Gestion de la mémoire GPU\n",
        "\n",
        "Jusqu'à présent, nous avons utilisé des tableaux NumPy sur le processeur comme entrées et sorties pour nos fonctions GPU. Pour plus de commodité, Numba a transféré automatiquement ces données au GPU pour que celui-ci puisse les exploiter. Avec ce transfert de données implicite, Numba, agissant de manière conservatrice, transférera automatiquement les données au processeur après le traitement. Comme vous pouvez l'imaginer, il s'agit d'une opération qui prend beaucoup de temps.\n",
        "\n",
        "Le [Guide des meilleures pratiques CUDA](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html) indique :\n",
        "\n",
        "> **Haute priorité** : minimiser le transfert de données entre l'hôte et l'appareil, même si cela signifie exécuter certains noyaux sur l'appareil qui ne présentent pas de gains de performances par rapport à leur exécution sur le processeur hôte.\n",
        "\n",
        "Dans cette optique, nous devons réfléchir à la manière d'empêcher ce transfert automatique de données vers l'hôte afin de pouvoir effectuer un travail supplémentaire sur les données, en ne payant le prix de leur recopie vers l'hôte que lorsque nous sommes vraiment prêts.\n",
        "\n",
        "Pour ce faire, nous créons des **tableaux de périphériques CUDA** et les transmettons à nos fonctions GPU. Les tableaux de périphériques ne seront pas automatiquement transférés vers l'hôte après le traitement et pourront être réutilisés comme nous le souhaitons sur le périphérique avant de les renvoyer, en tout ou en partie, vers l'hôte, et seulement si nécessaire.\n",
        "\n",
        "Pour démontrer, créons à nouveau notre exemple d'addition ufunc :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5LIS4TGztagG"
      },
      "outputs": [],
      "source": [
        "@vectorize(['float32(float32, float32)'], target='cuda')\n",
        "def add_ufunc(x, y):\n",
        "    return x + y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "xlLBl79utagG"
      },
      "outputs": [],
      "source": [
        "n = 100000\n",
        "x = np.arange(n).astype(np.float32)\n",
        "y = 2 * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pYTP4p0UtagG"
      },
      "outputs": [],
      "source": [
        "%timeit add_ufunc(x, y)  # Baseline performance with host arrays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR_zbAJrtagG"
      },
      "source": [
        "Le module `numba.cuda` inclut une fonction qui copiera les données de l'hôte sur le GPU et renverra un tableau de périphériques CUDA.\n",
        "\n",
        "Notez que lorsque nous essayons d'imprimer le contenu du tableau de périphériques, nous obtenons uniquement des informations sur le tableau, et non son contenu réel. Cela est dû au fait que les données se trouvent sur le périphérique et que nous devons les retransférer vers l'hôte afin d'imprimer ses valeurs, ce que nous montrerons comment faire plus tard :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "39l4VXTytagG"
      },
      "outputs": [],
      "source": [
        "from numba import cuda\n",
        "\n",
        "x_device = cuda.to_device(x)\n",
        "y_device = cuda.to_device(y)\n",
        "\n",
        "print(x_device)\n",
        "print(x_device.shape)\n",
        "print(x_device.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HePSM5yItagH"
      },
      "source": [
        "Les tableaux de périphériques peuvent être transmis aux fonctions CUDA tout comme les tableaux NumPy, mais sans la surcharge de copie :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "N2I__sBKtagH"
      },
      "outputs": [],
      "source": [
        "%timeit add_ufunc(x_device, y_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmLqVhZbtagH"
      },
      "source": [
        "Étant donné que `x_device` et `y_device` sont déjà présents sur le périphérique, ce test est beaucoup plus rapide.\n",
        "\n",
        "Nous allouons toujours un tableau de périphériques pour la sortie de l'ufunc et le copions sur l'hôte, même si dans la cellule ci-dessus nous n'assignons pas réellement le tableau à une variable. Pour éviter cela, nous pouvons créer le tableau de sortie avec la fonction [`numba.cuda.device_array()`](https://numba.pydata.org/numba-doc/dev/cuda-reference/memory.html#numba.cuda.device_array) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "M2nD0AoMtagH"
      },
      "outputs": [],
      "source": [
        "out_device = cuda.device_array(shape=(n,), dtype=np.float32)  # does not initialize the contents, like np.empty()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "fvvcD365tagH"
      },
      "source": [
        "Et puis nous pouvons utiliser un argument de mot-clé spécial « out » pour l'ufunc pour spécifier le tampon de sortie :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "HnxZkYW7tagH"
      },
      "outputs": [],
      "source": [
        "%timeit add_ufunc(x_device, y_device, out=out_device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyeafB1wtagH"
      },
      "source": [
        "Cet appel à `add_ufunc` n'implique aucun transfert de données entre l'hôte et le périphérique et s'exécute donc le plus rapidement. Si et quand nous voulons ramener un tableau de périphériques dans la mémoire de l'hôte, nous pouvons utiliser la méthode `copy_to_host()` :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nrXaroUetagH"
      },
      "outputs": [],
      "source": [
        "out_host = out_device.copy_to_host()\n",
        "print(out_host[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps6lXbygtagH"
      },
      "source": [
        "Vous pensez peut-être que nous ne comparons pas ici des pommes avec des pommes puisque nous n'avons pas évalué les appels `to_device` lors de l'utilisation des tableaux de périphériques bien que les transferts de données implicites soient comptabilisés dans l'évaluation lorsque nous utilisons les tableaux d'hôtes `a` et `b`, et vous auriez raison. Bien entendu, notre fonction `add_func` n'est pas particulièrement bien adaptée au GPU, l'exemple n'avait pour but que de démontrer comment les transferts peuvent être éliminés.\n",
        "\n",
        "Assurez-vous d'évaluer vos transferts de données lorsque vous déterminez si un passage au GPU en vaut la peine.\n",
        "\n",
        "De plus, Numba fournit des méthodes supplémentaires pour gérer la mémoire de l'appareil et le transfert de données, consultez [la documentation](https://numba.pydata.org/numba-doc/dev/cuda/memory.html) pour plus de détails."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1_8N8eHtagH"
      },
      "source": [
        "### Exercice : Optimiser le mouvement de la mémoire\n",
        "\n",
        "Soit donné ces ufuncs :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TJJ4NpD1tagH"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "@vectorize(['float32(float32, float32, float32)'], target='cuda')\n",
        "def make_pulses(i, period, amplitude):\n",
        "    return max(math.sin(i / period) - 0.3, 0.0) * amplitude\n",
        "\n",
        "n = 100000\n",
        "noise = (np.random.normal(size=n) * 3).astype(np.float32)\n",
        "t = np.arange(n, dtype=np.float32)\n",
        "period = n / 23"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZLkQfoktagH"
      },
      "source": [
        "Dans la cellule ci-dessous, il y a actuellement un aller-retour inutile de données vers l'hôte, puis de nouveau vers le périphérique, entre les appels à `make_pulses` et `add_ufunc`.\n",
        "\n",
        "Mettez à jour la cellule ci-dessous pour utiliser les allocations de périphériques afin qu'il n'y ait qu'une seule copie vers le périphérique avant l'appel à `make_pulses` et une seule copie vers l'hôte après l'appel à `add_ufunc`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5Ay7xZ9-tagH"
      },
      "outputs": [],
      "source": [
        "pulses = make_pulses(t, period, 100.0)\n",
        "waveform = add_ufunc(pulses, noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZevIE-ZJtagH"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(waveform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qID7QmUetagH"
      },
      "source": [
        "## Exercice final"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9L2a75aJtagI"
      },
      "source": [
        "Ce sujet de TD/TP fait partie de la certification \"Fundamentals of Accelerated Computing with CUDA Python\" de NVidia. On vous encourage à effectuer cet exercice et garder la réponse, ça vous permettra plus tard d'obtenir la certification (il y a 3 modules à compléter, celui-ci est le premier).\n",
        "\n",
        "**Veuillez lire attentivement les instructions avant de commencer votre travail pour garantir les meilleures chances de réussir l'évaluation.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HinAMcktagI"
      },
      "source": [
        "### Accélérez les calculs de réseaux neuronaux\n",
        "\n",
        "Vous allez refactoriser une version simple d'un code qui effectue le travail nécessaire pour créer une couche cachée dans un réseau neuronal. Il normalise les valeurs de niveaux de gris, les pondère et applique une fonction d'activation.\n",
        "\n",
        "Votre tâche consiste à déplacer ce travail vers le GPU en utilisant les techniques que vous avez apprises tout en préservant l'exactitude des calculs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOFKojVctagI"
      },
      "source": [
        "### Charger les imports et initialiser les valeurs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9AsehdDtagI"
      },
      "source": [
        "Exécutez cette cellule pour importer les bibliothèques requises et initialiser les valeurs avant de commencer votre travail ci-dessous."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "mitmHSr8tagI"
      },
      "outputs": [],
      "source": [
        "# You should not modify this cell, it contains imports and initial values needed to do work on either\n",
        "# the CPU or the GPU.\n",
        "\n",
        "import numpy as np\n",
        "from numba import cuda, vectorize\n",
        "\n",
        "# Our hidden layer will contain 1M neurons.\n",
        "# When you assess your work below, this value will be automatically set to 100M.\n",
        "n = 1000000\n",
        "\n",
        "greyscales = np.floor(np.random.uniform(0, 255, n).astype(np.float32))\n",
        "weights = np.random.normal(.5, .1, n).astype(np.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TbkEWa57tagI"
      },
      "source": [
        "### Accélération GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fc-c3LBdtagI"
      },
      "source": [
        "Vous devrez apporter des modifications à chacune des 3 cellules de cette section avant d'évaluer votre travail ci-dessous. Suivez les instructions dans les commentaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nn1dBbjmtagI"
      },
      "outputs": [],
      "source": [
        "# As you will recall, `numpy.exp` works on the CPU, but, cannot be used in GPU implmentations.\n",
        "# This import will work for the CPU-only boilerplate code provided below, but\n",
        "# you will need to modify this import before your GPU implementation will work.\n",
        "from numpy import exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "J17iS0vEtagI"
      },
      "outputs": [],
      "source": [
        "# Modify these 3 function calls to run on the GPU.\n",
        "def normalize(grayscales):\n",
        "    return grayscales / 255\n",
        "\n",
        "def weigh(values, weights):\n",
        "    return values * weights\n",
        "\n",
        "def activate(values):\n",
        "    return ( exp(values) - exp(-values) ) / ( exp(values) + exp(-values) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "TD2EFksXtagI"
      },
      "outputs": [],
      "source": [
        "# Modify the body of this function to optimize data transfers and therefore speed up performance.\n",
        "# As a constraint, even after you move work to the GPU, make this function return a host array.\n",
        "def create_hidden_layer(n, greyscales, weights, exp, normalize, weigh, activate):\n",
        "\n",
        "    normalized = normalize(greyscales)\n",
        "    weighted = weigh(normalized, weights)\n",
        "    activated = activate(weighted)\n",
        "\n",
        "    # The assessment mechanism will expect `activated` to be a host array, so,\n",
        "    # even after you refactor this code to run on the GPU, make sure to explicitly copy\n",
        "    # `activated` back to the host.\n",
        "    return activated"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVWfFsmetagI"
      },
      "source": [
        "### Validez votre travail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QfmZiWojtagI"
      },
      "source": [
        "Utilisez cette section pour tester et débuguer votre code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Ojj3255PtagI"
      },
      "outputs": [],
      "source": [
        "# You probably don't need to edit this cell, unless you change the name of any of the values being passed as\n",
        "# arguments to `create_hidden_layer` below.\n",
        "arguments = {\"n\":n,\n",
        "            \"greyscales\": greyscales,\n",
        "            \"weights\": weights,\n",
        "            \"exp\": exp,\n",
        "            \"normalize\": normalize,\n",
        "            \"weigh\": weigh,\n",
        "            \"activate\": activate}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12RdLWc-tagI"
      },
      "outputs": [],
      "source": [
        "# Use this cell (and feel free to create others) to self-assess your function\n",
        "a = create_hidden_layer(**arguments)\n",
        "print(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2YDzvTBtagI"
      },
      "source": [
        "### Soumettre le code à NVIDIA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17_3_6LetagJ"
      },
      "source": [
        "Enregistrer les cellules que vous avez modifié. L'enseignant vous montrera comment créer un compte Nvidia Developper afin de vous inscrire dans la certification et soumettre votre réponse."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}